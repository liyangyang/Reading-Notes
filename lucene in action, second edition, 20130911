Lucene in Action 2en Edition
Part 1: Core Lucene
Meet Lucene
	Dealing with Information explosion
	What is Lucene?
		IR: information retrieval
		What Lucene can do
		History of Lucene
	Lucene and Component of a Search Application
		Components for Indexing
			Nearly all search engines, including Lucene, automatically statically boost fields that are shorter over fields that are longer. Intuitively this makes sense: if you match a word or two in a very long document, it’s quite a bit less relevant than matching the same words in a document that’s, say, three or four words long.
			No search engine indexes text directly: rather, the text must be broken into a series of individual atomic elements called tokens. This is what happens during the Analyze Document step. Each token corresponds roughly to a “word” in the language, and this step determines how the textual fields in the document are divided into a series of tokens.
			Lucene provides an array of built-in analyzers that give you fine control over this process. It’s also straightforward to build your own analyzer, or create arbitrary analyzer chains combining Lucene’s tokenizers and token filters, to customize how tokens are created.
			It’s important to remember that indexing is something of a necessary evil that you must undertake in order to provide a good search experience: you should design and customize your indexing process only to the extent that you improve your users’ search experience.
		Components for Searching
			The quality of a search is typically described using precision and recall metrics. Recall measures how well the search system finds relevant documents; precision measures how well the system filters out the irrelevant documents.
			There are three common theoretical models of search:
				?? Pure Boolean model—Documents either match or don’t match the provided query, and no scoring is done. In this model there are no relevance scores associated with matching documents, and the matching documents are unordered; a query simply identifies a subset of the overall corpus as matching the query.
				?? Vector space model—Both queries and documents are modeled as vectors in a high dimensional space, where each unique term is a dimension. Relevance, or similarity, between a query and a document is computed by a vector distance measure between these vectors.
				?? Probabilistic model—In this model, you compute the probability that a document is a good match to a query using a full probabilistic approach.
			Lucene’s approach combines the vector space and pure Boolean models, and offers you controls to decide which model you’d like to use on a search-by-search basis.
		The Rest of Search Application
			ADMINISTRATION INTERFACE
			ANALYTICS INTERFACE
				Analytics is important: you can gain a lot of intelligence about your users and why they do or do not buy your widgets through your website, by looking for patterns in the search logs.
				Lucene-specific metrics that could feed the analytics interface include:
					?? How often which kinds of queries (single term, phrase, Boolean queries, etc.) are run
					?? Queries that hit low relevance
					?? Queries where the user didn’t click on any results (if your application tracks click-throughs)
					?? How often users are sorting by specified fields instead of relevance
					?? The breakdown of Lucene’s search time
				Lucene, since it’s a search library, doesn’t provide any analytics tools. If your search application is web based, Google Analytics is a fast way to create an analytics interface. If that doesn’t fit your needs, you can also build your own custom charts based on Google’s visualization API.
			SCALING
				There are two dimensions to scaling: net amount of content, and net query throughput. If you have a tremendous amount of content, you must divide it into shards, so that a separate computer searches each shard. A front-end server sends a single incoming query to all shards, and then coalesces the results into a single result set. If instead you have high search throughput during your peak traffic, you’ll have to take the same index and replicate it across multiple computers. A front-end load balancer sends each incoming query to the least loaded back-end computer. If you require both dimensions of scaling, as a web scale search engine will, you combine both of these practices.
				Lucene provides no facilities for scaling. However, both Solr and Nutch, projects under the Apache Lucene umbrella, provide support for index sharding and replication. The Katta open source project, hosted at http://katta.sourceforge.net and based on Lucene, also provides this functionality. Elastic search, at http://www.elasticsearch. com, is another option that’s also open source and based on Lucene.
		Where Lucene fits into Your Application
	Lucene in Action: A Sample Application
		Creating An Index
		Searching An Index
			Note that the TopDocs object contains only references to the underlying documents. In other words, instead of being loaded immediately upon search, matches are loaded from the index in a lazy fashion—only when requested with the Index- Searcher.doc(int) call. That call returns a Document object from which we can then retrieve individual field values.
		Understanding the core indexing classes	
			IndexWriter
				IndexWriter creates a new index or opens an existing one, and adds, removes, or updates documents in the index.
			Directory
			Analyzer
			Document
				Lucene only deals with text and numbers. Lucene’s core doesn’t itself handle anything but java.lang.String, java.io.Reader, and native numeric types (such as int or float).
			Field
				Each field has a name and corresponding value, and a bunch of options, that control precisely how Lucene will index the field’s value.
	Understanding the core searching classes
		IndexSearcher
		Term
			A Term is the basic unit for searching. Similar to the Field object, it consists of a pair of string elements: the name of the field and the word (text value) of that field. Note that Term objects are also involved in the indexing process. However, they’re created by Lucene’s internals, so you typically don’t need to think about them while indexing. During searching, you may construct Term objects and use them together with TermQuery
		Query
			It contains several utility methods, the most interesting of which is setBoost(float), which enables you to tell Lucene that certain subqueries should have a stronger contribution to the final relevance score than other subqueries.
		TermQuery
			It’s used for matching documents that contain fields with specific values
		TopDocs
			The TopDocs class is a simple container of pointers to the top N ranked search results—documents that match a given query. For each of the top N results, TopDocs records the int docID (which you can use to retrieve the document) as well as the float score.
	Summary
	
	
Building a Search Index
	How Lucene Models Content
		Documents and Fields
			At a high level, there are three things Lucene can do with each field:
				?? The value may be indexed (or not). A field must be indexed if you intend to search on it. Only text fields may be indexed (binary valued fields may only be stored). When a field is indexed, tokens are first derived from its text value, using a process called analysis, and then those tokens are enrolled into the index. See section 2.4.1 for options that control how the field’s value is indexed.
				?? If it’s indexed, the field may also optionally store term vectors, which are collectively a miniature inverted index for that one field, allowing you to retrieve all of its tokens. This enables certain advanced use cases, like searching for documents similar to an existing one (more uses are covered in section 5.7). See section 2.4.3 for options that control how term vectors are indexed. Licensed to theresa smith <anhvienls@gmail.com> How Lucene models content 33
				?? Separately, the field’s value may be stored, meaning a verbatim copy of the unanalyzed value is written away in the index so that it can later be retrieved. This is useful for fields you’d like to present unchanged to the user, such as the document’s title or abstract. See section 2.4.2 for options that control how the field’s values are stored.
			When you retrieve a document from the index, only the stored fields will be present. For example, fields that were indexed but not stored won’t be in the document.
		Flexible schema
			Unlike a database, Lucene has no notion of a fixed global schema. In other words, each document you add to the index is a blank slate and can be completely different from the document before it: it can have whatever fields you want, with any indexing and storing and term vector options. It need not have the same fields as the previous document you added. It can even have the same fields, with different options, than in other documents.
		Denormalization
	Understanding the Indexing process
		Extracting text and creating the document
		Analysis
			Lucene first analyzes the text, a process that splits the textual data into a stream of tokens, and performs a number of optional operations on them. The combination of an original source of tokens, followed by the series of filters that modify the tokens produced by that source, make up the analyzer.
		Adding to the index
			Lucene stores the input in a data structure known as an inverted index. This data structure makes efficient use of disk space while allowing quick keyword lookups.
			Every Lucene index consists of one or more segments, as depicted in figure 2.2. Each segment is a standalone index, holding a subset of all indexed documents. A new segment is created whenever the writer flushes buffered added documents and pending deletions into the directory. At search time, each segment is visited separately and the results are combined.
			Each segment, in turn, consists of multiple files, of the form _X.<ext>, where X is the segment’s name and <ext> is the extension that identifies which part of the index that file corresponds to. There are separate files to hold the different parts of the index (term vectors, stored fields, inverted index, and so on). If you’re using the compound file format (which is enabled by default but you can change using Index- Writer.setUseCompoundFile), then most of these index files are collapsed into a single compound file: _X.cfs. This reduces the number of open file descriptors during searching, at a small cost of searching and indexing performance. Chapter 11 covers this trade-off in more detail.
			There’s one special file, referred to as the segments file and named segments_<N>, that references all live segments. This file is important! Lucene first opens this file, and then opens each segment referenced by it. The value <N>, called “the generation,” is an integer that increases by one every time a change is committed to the index. 
			Naturally, over time the index will accumulate many segments, especially if you open and close your writer frequently. This is fine. Periodically, IndexWriter will select segments and coalesce them by merging them into a single new segment and then removing the old segments. The selection of segments to be merged is governed by a separate MergePolicy. Once merges are selected, their execution is done by the MergeScheduler. These classes are advanced topics, covered in section 2.13.6.
	Basic Index Operation
		Adding documents to an index
			There are two methods for adding documents: 
				?? addDocument(Document)—Adds the document using the default analyzer, which you specified when creating the IndexWriter, for tokenization.
				?? addDocument(Document, Analyzer)—Adds the document using the provided analyzer for tokenization. But be careful! In order for searches to work correctly, you need the analyzer used at search time to “match” the tokens produced by the analyzers at indexing time. See section 4.1.2 for more details.
		Deleting documents from an index
			IndexWriter provides various methods to remove documents from an index:
				?? deleteDocuments(Term) deletes all documents containing the provided term.
				?? deleteDocuments(Term[])deletes all documents containing any of the terms in the provided array.
				?? deleteDocuments(Query) deletes all documents matching the provided query.
				?? deleteDocuments(Query[])deletes all documents matching any of the queries in the provided array.
				?? deleteAll() deletes all documents in the index. This is exactly the same as closing the writer and opening a new writer with create=true, without having to close your writer.
			In each case, the deletes aren’t done immediately. Instead, they’re buffered in memory, just like the added documents, and periodically flushed to the directory. As with added documents, you must call commit() or close() on your writer to commit the changes to the index. Even once the deletes are flushed to the directory, the disk space consumed by that document isn’t immediately freed. Rather, the documents are simply marked as deleted.
			hasDeletions() method to check if an index contains any documents marked for deletion.
			Users often confuse the maxDoc() and numDocs() methods in Index- Writer and IndexReader. The first method, maxDoc() returns the total number of deleted or undeleted documents in the index, whereas num- Docs() returns only the number of undeleted documents.
			We force Lucene to merge index segments, after deleting one document, by optimizing the index(writer.optimize()). Then, the maxDoc() method returns 1 rather than 2, because after a delete and optimize, Lucene truly removes the deleted document. Only one document remains in the index.
		Updating documents in the index
			In some cases you may want to update only certain fields of the document. Perhaps the title changed but the body was unchanged. Unfortunately, although this is a frequently requested feature, Lucene can’t do that: instead, it deletes the entire previous document and then adds a new document to the index. This requires that the new document contains all fields, even unchanged ones, from the original document. IndexWriter provides two convenience methods to replace a document in the index:
				?? updateDocument(Term, Document)first deletes all documents containing the provided term and then adds the new document using the writer’s default analyzer.
				?? updateDocument(Term, Document, Analyzer) does the same but uses the provided analyzer instead of the writer’s default analyzer.
			Note that these methods are simply shorthand for first calling deleteDocuments(Term) and then addDocument.
	Field Options
		When you create a field, you can specify numerous options to control what Lucene should do with that field once you add the document to the index.
		Field Options for indexing
			The options for indexing (Field.Index.*) control how the text in the field will be made searchable via the inverted index.
				?? Index.ANALYZED—Use the analyzer to break the field’s value into a stream of separate tokens and make each token searchable. This option is useful for normal text fields (body, title, abstract, etc.).
				?? Index.NOT_ANALYZED—Do index the field, but don’t analyze the String value. Instead, treat the Field’s entire value as a single token and make that token searchable. This option is useful for fields that you’d like to search on but that shouldn’t be broken up, such as URLs, file system paths, dates, personal names, Social Security numbers, and telephone numbers. This option is especially useful for enabling “exact match” searching. We indexed the id field in listings 2.1 and 2.3 using this option.
				?? Index.ANALYZED_NO_NORMS—A variant of Index.ANALYZED that doesn’t store norms information in the index. Norms record index-time boost information in the index but can be memory consuming when you’re searching. Section 2.5.3	describes norms in detail.
				?? Index.NOT_ANALYZED_NO_NORMS—Just like Index.NOT_ANALYZED, but also doesn’t store norms. This option is frequently used to save index space and memory usage during searching, because single-token fields don’t need the norms information unless they’re boosted.
				?? Index.NO—Don’t make this field’s value available for searching.
			When Lucene builds the inverted index, by default it stores all necessary information to implement the Vector Space Model. This model requires the count of every term that occurred in the document, as well as the positions of each occurrence (needed, for example, by phrase searches). But sometimes you know the field will be used only for pure Boolean searching and need not contribute to the relevance score. Fields used only for filtering, such as entitlements or date filtering, are a common example.
			In this case, you can tell Lucene to skip indexing the term frequency and positions by calling Field.setOmitTermFreqAndPositions(true). This approach will save some disk space in the index, and may also speed up searching and filtering, but will silently prevent searches that require positional information, such as PhraseQuery and Span- Query, from working.
		Field Options for stroing fields
			The options for stored fields (Field.Store.*) determine whether the field’s exact value should be stored away so that you can later retrieve it during searching:
				?? Store.YES—Stores the value. When the value is stored, the original String in its entirety is recorded in the index and may be retrieved by an IndexReader. This option is useful for fields that you’d like to use when displaying the search results (such as a URL, title, or database primary key). Try not to store very large fields, if index size is a concern, as stored fields consume space in the index.
				?? Store.NO—Doesn’t store the value. This option is often used along with Index.ANALYZED to index a large text field that doesn’t need to be retrieved in its original form, such as bodies of web pages, or any other type of text document.
			Lucene includes a helpful utility class, CompressionTools, that exposes static methods to compress and decompress byte arrays. Under the hood it uses Java’s built-in java.util.Zip classes. You can use CompressionTools to compress values before storing them in Lucene. Note that although doing so will save space in your index, depending on how compressible the content is, it will also slow down indexing and searching. You’re spending more CPU in exchange for less disk space used, which for many applications isn’t a good trade-off. If the field values are small, compression is rarely worthwhile.
		Field Options for term vectors
			Term vectors are a mix between an indexed field and a stored field. They’re similar to a stored field because you can quickly retrieve all term vector fields for a given document: term vectors are keyed first by document ID. But then, they’re keyed secondarily by term, meaning they store a miniature inverted index for that one document. Unlike a stored field, where the original String content is stored verbatim, term vectors store the actual separate terms that were produced by the analyzer, allowing you to retrieve all terms for each field, and the frequency of their occurrence within the document, sorted in lexicographic order. Because the tokens coming out of an analyzer also have position and offset information (see section 4.2.1), you can choose separately whether these details are also stored in your term vectors by passing these constants as the fourth argument to the Field constructor:
				?? TermVector.YES—Records the unique terms that occurred, and their counts, in each document, but doesn’t store any positions or offsets information
				?? TermVector.WITH_POSITIONS—Records the unique terms and their counts, and also the positions of each occurrence of every term, but no offsets
				?? TermVector.WITH_OFFSETS—Records the unique terms and their counts, with the offsets (start and end character position) of each occurrence of every term, but no positions
				?? TermVector.WITH_POSITIONS_OFFSETS—Stores unique terms and their counts, along with positions and offsets
				?? TermVector.NO—Doesn’t store any term vector information
			Note that you can’t index term vectors unless you’ve also turned on indexing for the field. Stated more directly: if Index.NO is specified for a field, you must also specify TermVector.NO.
		Reader, TokenStream, and byte[] field values
			There are a few other constructors for the Field object that allow you to use values other than String:
				?? Field(String name, Reader value, TermVector termVector) uses a Reader instead of a String to represent the value. In this case, the value can’t be stored (the option is hardwired to Store.NO) and is always analyzed and indexed (Index.ANALYZED). This can be useful when holding the full String in memory might be too costly or inconvenient—for example, for very large values.
				?? Field(String name, Reader value), like the previous value, uses a Reader instead of a String to represent the value but defaults termVector to Term- Vector.NO.
				?? Field(String name, TokenStream tokenStream, TermVector termVector) allows you to preanalyze the field value into a TokenStream. Likewise, such fields aren’t stored and are always analyzed and indexed.
				?? Field(String name, TokenStream tokenStream), like the previous value, allows you to preanalyze the field value into a TokenStream but defaults termVector to TermVector.NO.
				?? Field(String name, byte[] value, Store store) is used to store a binary field. Such fields are never indexed (Index.NO) and have no term vectors (Term- Vector.NO). The store argument must be Store.YES.
				?? Field(String name, byte[] value, int offset, int length, Store store), like the previous value, indexes a binary field but allows you to reference a subslice of the bytes starting at offset and running for length bytes.
		Field Option Combinations
			Table 2.1 lists commonly used options and their example usage, but remember you are free to set the options however you’d like.
				Index					Store					TermVector						Example usage
				NOT_ANALYZED_NO_NORMS	YES 					NO 								Identifiers (filenames, primary keys), telephone and Social Security numbers, URLs, personal names, dates, and textual fields for sorting
				ANALYZED 				YES 					WITH_POSITIONS_OFFSETS 			Document title, document abstract
				ANALYZED 				NO 						WITH_POSITIONS_OFFSETS 			Document body
				NO 						YES 					NO 								Document type, database primary key (if not used for searching)
				NOT_ANALYZED 			NO 						NO 								Hidden keywords
		Field Options for sorting
			In order to perform field sorting, you must first index the fields correctly.
			If the field is numeric, use NumericField, covered in section 2.6.1, when adding it to the document, and sorting will work correctly. If the field is textual, such as the sender’s name in an email message, you must add it as a Field that’s indexed but not analyzed using Field.Index.NOT_ANALYZED. If you aren’t doing any boosting for the field, you should index it without norms, to save disk space and memory, using Field.Index.NOT_ANALYZED_NO_NORMS
			Fields used for sorting must be indexed and must contain one token per document. Typically this means using Field.Index.NOT_ANALYZED or Field.Index.NOT_ANALYZED_NO_NORMS (if you’re not boosting documents or fields), but if your analyzer will always produce only one token, such as KeywordAnalyzer (covered in section 4.7.3), Field.Index. ANALYZED or Field.Index.ANALYZED_NO_NORMS will work as well.
		Multivalued fields
			Besides Strings, field values can also be binary values (for storing), a TokenStream value (for preanalyzed fields), or a Reader (if holding the full String in memory is too costly or inconvenient).
			Internally, whenever multiple fields with the same name appear in one document, both the inverted index and term vectors will logically append the tokens of the field to one another, in the order the fields were added. You can use advanced options during analysis that control certain important details of this appending, notably how to prevent searches from matching across two different field values; see section 4.7.1 for details. But, unlike indexing, when the fields are stored they’re stored separately in order in the document, so that when you retrieve the document at search time you’ll see multiple Field instances.
	Boosting Documents and Fields
		Boosting may be done during indexing, as we describe here, or during searching, as described in section 5.7. Search-time boosting is more dynamic, because every search can separately choose to boost or not to boost with different factors, but also may be somewhat more CPU intensive. Because it’s so dynamic, search-time boosting also allows you to expose the choice to the user, such as a checkbox that asks “Boost recently modified documents?”.
		Boosting documents
			By default, all documents have no boost—or, rather, they all have the same boost factor of 1.0. By changing a document’s boost factor, you can instruct Lucene to consider it more or less important with respect to other documents in the index when computing relevance. The API for doing this consists of a single method, set- Boost(float)
		Boosting fields
			When you boost a document, Lucene internally uses the same boost factor to boost each of its fields.
			Field.setBoost(float)
			It’s worth noting that shorter fields have an implicit boost associated with them, due to the way Lucene’s scoring algorithm works. While indexing, IndexWriter consults the Similarity.lengthNorm method to perform this computation. To override this logic, you can implement your own Similarity class and tell IndexWriter to use it by calling its setSimilarity method. Boosting is, in general, an advanced feature that many applications can work well without, so tread carefully!
			How does Lucene record these boost factors in the index? This is what norms are for.
		Norms
			During indexing, all sources of index-time boosts are combined into a single floatingpoint number for each indexed field in the document. The document may have its own boost; each field may have a boost; and Lucene computes an automatic boost based on the number of tokens in the field (shorter fields have a higher boost). These boosts are combined and then compactly encoded (quantized) into a single byte, which is stored per field per document. During searching, norms for any field being searched are loaded into memory, decoded back into a floating-point number, and used when computing the relevance score.
			Even though norms are initially computed during indexing, it’s also possible to change them later using IndexReader’s setNorm method. setNorm is an advanced method that requires you to recompute your own norm factor, but it’s a potentially powerful way to factor in highly dynamic boost factors, such as document recency or click-through popularity.
			One problem often encountered with norms is their high memory usage at search time. This is because the full array of norms, which requires one byte per document per separate field searched, is loaded into RAM. For a large index with many fields per document, this can quickly add up to a lot of RAM. Fortunately, you can easily turn norms off by either using one of the NO_NORMS indexing options in Field.Index or by calling Field.setOmitNorms(true) before indexing the document containing that field. Doing so will potentially affect scoring, because no index-time boost information will be used during searching, but it’s possible the effect is trivial, especially when the fields tend to be roughly the same length and you’re not doing any boosting on your own.
			Beware: if you decide partway through indexing to turn norms off, you must rebuild the entire index because if even a single document has that field indexed with norms enabled, then through segment merging this will “spread” so that all documents consume one byte even if they’d disabled norms. This happens because Lucene doesn’t use sparse storage for norms.
	Indexing Numbers, Dates and Times
		Indexing numbers
			To enable this, simply pick an analyzer that doesn’t discard numbers, WhitespaceAnalyzer and StandardAnalyzer are two possible candidates.
			SimpleAnalyzer and StopAnalyzer discard numbers from the token stream, which means the search for 1099 won’t match any documents.
			Under the hood, Lucene works some serious magic to ensure numeric values are indexed to allow for efficient range searching and numeric sorting. Each numeric value is indexed using a trie structure, which logically assigns a single numeric value to larger and larger predefined brackets. Each bracket is assigned a unique term in the index, so that retrieving all documents within a single bracket is fast. At search time, the requested range is translated into an equivalent union of these brackets, resulting in a high-performance range search or filter.
			Although each NumericField instance accepts only a single numeric value, you’re allowed to add multiple instances, with the same field name, to the document. The resulting NumericRangeQuery and NumericRangeFilter will logically “or” together all the values. But the effect on sorting is undefined. If you require sorting by the field, you’ll have to index a separate NumericField that has only one occurrence for that field name.
			An advanced parameter, precisionStep, lets you control the gap (in bits) between each successive bracket. The default value is 4 bits. Smaller values result in more trie brackets, thus increasing the size of the index (usually by a minor amount) but allowing for potentially faster range searching. The Javadocs provide full details of these trade-offs, but likely the default value is sufficient for most applications.
		Indexing dates and times
			NumericField can also easily handle dates and times by converting them to equivalent ints or longs.
	Field truncation
		To support these diverse cases, IndexWriter allows you to truncate per-Field indexing so that only the first N terms are indexed for an analyzed field. When you instantiate IndexWriter, you must pass in a MaxFieldLength instance expressing this limit. MaxFieldLength provides two convenient default instances: MaxField- Length.UNLIMITED, which means no truncation will take place, and MaxField- Length.LIMITED, which means fields are truncated at 10,000 terms. You can also instantiate MaxFieldLength with your own limit.
		After creating IndexWriter, you may alter the limit at any time by calling setMax- FieldLength or retrieve the limit with getMaxFieldLength. However, any documents already indexed will have been truncated at the previous value: changes to maxField- Length aren’t retroactive. If multiple Field instances with the same name exist, the truncation applies separately to each of them, meaning each field has its first N terms indexed. If you’re curious about how often the truncation is kicking in, call Index- Writer.setInfoStream(System.out) and search for any lines that say "maxField- Length reached for field X, ignoring following tokens". (Note that the infoStream also receives many other diagnostic details, useful in their own right.)	
		Please think carefully before using any field truncation! It means that only the first N terms are available for searching, and any text beyond the Nth term is completely ignored. Searches that would’ve matched a document after the Nth term will silently fail to match the document.	
		Use maxFieldLength sparingly!	
	Near-real-time search
		The method IndexReader getReader() in IndexWriter. This method immediately flushes any buffered added or deleted documents, and then creates a new read-only IndexReader that includes those documents. We’ll see how IndexReader is used for searching in the next chapter, but for now, just trust us! Under the hood, the newly opened reader is instantiated in an efficient manner, so that any old segments in common with the previously opened reader are shared. Thus, if only a few documents have been added, the turnaround time will generally be fast. Note that calling getReader necessarily slows down your indexing throughput because it causes the IndexWriter to immediately flush a new segment instead of waiting until its RAM buffer is full.
	Optimizing an index
		When you index documents, especially many documents or using multiple sessions with IndexWriter, you’ll invariably create an index that has many separate segments. When you search the index, Lucene must search each segment separately and then combine the results. Although this works flawlessly, applications that handle large indexes will see search performance improvements by optimizing the index, which merges many segments down to one or a few segments. An optimized index also consumes fewer file descriptors during searching.
		Optimizing only improves searching speed, not indexing speed.
		It’s entirely possible that you get excellent search throughput without optimizing, so be sure to first test whether you need to consider optimizing.
		IndexWriter exposes four methods to optimize:
			?? optimize() reduces the index to a single segment, not returning until the operation is finished.
			?? optimize(int maxNumSegments), also known as partial optimize, reduces the index to at most maxNumSegments segments. Because the final merge down to one segment is the most costly, optimizing to, say, five segments should be quite a bit faster than optimizing down to one segment, allowing you to trade less optimization time for slower search speed. 
			?? optimize(boolean doWait) is just like optimize, except if doWait is false then the call returns immediately while the necessary merges take place in the background. Note that doWait=false only works for a merge scheduler that runs merges in background threads, such as the default ConcurrentMergeScheduler.
			?? optimize(int maxNumSegments, boolean doWait) is a partial optimize that runs in the background if doWait is false.
		Another important cost to be aware of is that optimizing requires substantial temporary disk space. Because Lucene must merge segments together, while the merge is running temporary disk space is used to hold the files for the new segment. But the old segments can’t be removed until the merge is complete and the changes are committed, either by calling IndexWriter.commit or by closing the IndexWriter. This means you should expect the size of your index to roughly triple (temporarily) during optimization. Once optimization completes, and once you call commit(), disk usage will fall back to a lower level than the starting size. Any open readers on the index will also potentially impact the transient disk usage.	
	Other directory implementations
		Of these classes, three are concrete Directory implementations to read and write files from the file system. They’re all subclasses of the abstract FSDirectory base class. Unfortunately, there’s no single best FSDirectory implementation. Each has potentially serious limitations in certain situations:
			?? SimpleFSDirectory uses java.io.* APIs for access. Unfortunately, this Directory implementation doesn’t scale during reading when multiple threads are in use because it must do internal locking to work around the lack of positional reads in java.io.*. 
			?? NIOFSDirectory uses positional reads in java.nio.* APIs, and thus has no internal locking and scales very well with many threads when reading. Unfortunately, due to a longstanding Windows-only issue on Sun’s JREs, NIOFSDirectory will perform badly, perhaps worse than SimpleFSDirectory, on Windows.
			?? MMapDirectory uses memory-mapped I/O when reading and also doesn’t have any locking, so it scales well with threads. But because memory-mapped I/O consumes process address space equal to the size of your index, it’s best to use it only on a 64-bit JRE, or on a 32-bit JRE if you’re absolutely certain that your index size is very small relative to the actual portion of 32-bit address space available to your process (typically 2–3 GB, depending on the OS). Java doesn’t provide a way to cleanly “unmap” memory-mapped sections of a file, which means it’s only when garbage collection happens that the underlying file is closed and memory is freed. This means you could easily have many leftover maps, consuming large chunks of process address space and leaving the underlying index files open far longer than you’d expect. Furthermore, on 32-bit JREs, you may hit false OutOfMemoryError due to fragmentation issues. MMap- Directory provides the setMaxChunkSize method to work around this.
		So which implementation should you use? One good approach is to use the static FSDirectory.open method. This method attempts to pick the best default FSDirectory implementation given your current OS and platform, and may improve its decision making with each Lucene release (though note that as of Lucene 3.0, it won’t ever return an MMapDirectory). Alternatively, you can directly instantiate the precise class that you want, as long as you understand the previous issues (be sure to read the Javadocs for all the latest details!).
		But if the computer has enough RAM, most OSs will use free RAM as an I/O cache. This means, after warming up, the FSDirectory will be about as fast as the RAMDirectory for searching.
		The final Directory implementation, FileSwitchDirectory, switches between two Directory implementations you provide, based on the extension of the file. This implementation could be used to store certain index files in a RAMDirectory and others in a backing MMapDirectory, for example. But realize this is an advanced use and you must rely on the extensions of the current Lucene index file format, which is free to change between releases.
		A more general API is this static method, to copy all files between any two Directory instances: Directory.copy(Directory sourceDir, Directory destDir, boolean closeDirSrc); But be aware that this blindly replaces any existing files in destDir, and you must ensure no IndexWriter is open on the source directory because the copy method doesn’t do any locking
		If the destDir already has an index present and you’d like to add in all documents from srcDir, keeping all documents already indexed in otherDir, use IndexWriter.addIndexesNoOptimize instead: IndexWriter writer = new IndexWriter(otherDir, analyzer, IndexWriter.MaxFieldLength.UNLIMITED); writer.addIndexesNoOptimize(new Directory[] {ramDir});
	Concurrency, thread-safety and locking issues
		Thread and multi-JVM safety
			Lucene’s concurrency rules are simple:
				?? Any number of read-only IndexReaders may be open at once on a single index. It doesn’t matter if these readers are in the same JVM or multiple JVMs, or on the same computer or multiple computers. Remember, that within a single JVM it’s best for resource utilization and performance reasons to share a single IndexReader instance for a given index using multiple threads. For instance, multiple threads or processes may search the same index in parallel.
				?? Only a single writer may be open on an index at once. Lucene uses a write lock file to enforce this (described in detail in section 2.11.3). As soon as an Index- Writer is created, a write lock is obtained. Only when that IndexWriter is closed is the write lock released. Note that if you use IndexReader to make changes to the index—for example, to change norms (section 2.5.3) or delete documents (section 2.13.1)—then that IndexReader acts as a writer: it must successfully obtain the write lock before making the first change, only releasing it once closed.
				?? IndexReaders may be open even while an IndexWriter is making changes to the index. Each IndexReader will always show the index as of the point in time that it was opened. It won’t see any changes being done by the IndexWriter until the writer commits and the reader is reopened. It’s even fine to open a new IndexWriter with create=true while an IndexReader is already open: that IndexReader will continue searching its point-in-time view of the index.
				?? Any number of threads can share a single instance of IndexReader or Index- Writer. These classes are not only thread safe but also thread friendly, meaning they generally scale well as you add threads (assuming your hardware has concurrency, because the amount of synchronized code inside these classes is kept to a minimum).
		Accessing an index over a remote file system
			It’s possible to gain some performance back by mounting the remote file system as read-only, but to maximize performance it’s best to replicate a copy of the index onto the local file system of each computer that will do searching.
			As you’ve seen, Lucene allows highly concurrent access to an index. Many readers can share an index, many threads can share an IndexWriter and IndexReader, and so forth. The only strong concurrency limitation is that no more than one writer may be open at once.
			If you still intend to access the index over a remote file system, it’s important to be aware of the possible limitations. Unfortunately, certain popular remote file systems are known to be problematic, as summarized in table 2.3. NFS, AFP, and Samba/CIFS 2.0 are known to have intermittent problems when opening or reopening an index due to incoherent client-side caching. The problem only occurs when the writer has just committed changes to an index, and then on another computer a reader or another writer is opened or reopened. Thus you’re more likely to encounter this if you frequently try to reopen your readers and writer and often commit changes to the index. When you do encounter the issue, you’ll see an unexpected FileNotFound- Exception inside the open or reopen methods. Fortunately, the workaround is quite simple: retry a bit later, because typically the client-side caches will correct themselves after a certain amount of time.
			NFS in particular presents a further challenge because of how it handles deletion of files that are still held open on other computers. Most file systems protect open files from deletion. For example, Windows simply disallows deletion of an open file, whereas most native Unix file systems allow the deletion to proceed but the actual bytes of the file remain allocated on disk until all open file handles are closed (this is called “delete on last close” semantics). In both approaches, an open file handle can still be used to read all bytes in the file after the file deletion is attempted. NFS does neither of these, and simply removes the file, so that the next I/O operation attempted by a computer with an open file handle will encounter the much-dreaded “Stale NFS file handle” IOException.
			To prevent this error from hitting your searchers, you must create your own IndexDeletionPolicy class to control deletion of previous commit points until all searchers on the index have reopened to the newer commit point. For example, a common approach is to remove an index commit only if it’s older than, say, 4 hours, as long as you can ensure that every IndexReader reading the index reopens itself less than 4 hours after a commit. Alternatively, on hitting the “Stale NFS file handle” during searching, you could at that moment reopen your searcher and then redo the search. This is a viable approach only if reopening a searcher is not too time consuming. Otherwise, the unlucky query that hit the error will take unacceptably long to get results.
		Index locking
			Lucene uses a file-based lock: if the lock file (write.lock by default) exists in your index directory, a writer currently has the index open. Any attempt to create another writer on the same index will hit a LockObtain- FailedException.
			Lucene allows you to change your locking implementation: any subclass of Lock- Factory can be set as your locking implementation by calling Directory.setLock- Factory. Be sure to call this before opening an IndexWriter on that Directory instance. Normally you don’t need to worry about which locking implementation you’re using. It’s usually only those advanced applications that have multiple computers or JVMs that take turns performing indexing that may need to customize the locking implementation.
			Locking implementations provided by Lucene:
				Locking class name 										Description
				NativeFSLockFactory 									This is the default locking for FSDirectory, using java.nio native OS locking, which will never leave leftover lock files when the JVM exits. But this locking implementation may not work correctly over certain shared file systems, notably NFS.
				SimpleFSLockFactory 									Uses Java’s File.createNewFile API, which may be more portable across different file systems than NativeFSLockFactory. Be aware that if the JVM crashes or IndexWriter isn’t closed before the JVM exits, this may leave a leftover write.lock file, which you must manually remove. 
				SingleInstanceLockFactory 								Creates a lock entirely in memory. This is the default locking implementation for RAMDirectory. Use this when you know all IndexWriters will be instantiated in a single JVM.
				NoLockFactory 											Disables locking entirely. Be careful! Only use this when you are absolutely certain that Lucene’s normal locking safeguard isn’t necessary—for example, when using a private RAMDirectory with a single IndexWriter instance.
			Note that none of these locking implementations are “fair.” For example, if a lock is already held by an existing writer, the new writer will simply retry, every one second by default, to obtain the lock. There’s no queue that would allow the new writer to get the lock as soon as the old one releases it. If you have an application that requires such fairness, it’s best to implement your own locking.
			If you do choose to create your own locking implementation, be certain it works correctly. There’s a simple but useful debugging tool, LockStressTest, which can be used in conjunction with LockVerifyServer and VerifyingLockFactory to verify that a given locking implementation is functioning properly. These classes are in the org.apache.lucene.store package
			You should be aware of two additional methods related to locking:
				?? IndexWriter’s isLocked(Directory)—Tells you whether the index specified in its argument is locked. This method can be handy when an application needs to check whether the index is locked before attempting to create an IndexWriter.
				?? IndexWriter’s unlock(Directory)—Does exactly what its name implies. Although this method gives you power to unlock any Lucene index at any time, using it is dangerous. Lucene creates locks for a good reason, and unlocking an index while it’s being modified will quickly result in a corrupted and unusable index.
			Although you now know about Lucene’s write lock, you should resist touching this file directly. Instead, always rely on Lucene’s API to manipulate it. If you don’t, your code may break if Lucene starts using a different locking mechanism in the future, or even if it changes the name or location of its lock files.
	Debugging indexing		
		If you ever need to debug Lucene’s index-writing process, remember that you can get Lucene to output information about its indexing operations by calling IndexWriter’s setInfoStream method, passing in a PrintStream such as System.out	
	Advanced indexing concepts
		Deleting documents with IndexReader
			IndexReader also exposes methods to delete documents. Why would you want two ways to do the same thing? Well, there are some interesting differences:
				?? IndexReader is able to delete by document number. This means you could do a search, step through matching document numbers, perhaps apply some applicationlogic, then pick and choose which document numbers to delete. Although frequently requested, IndexWriter can’t expose such a method because document numbers may change suddenly due to merging (see section 2.13.6).
				?? IndexReader can delete by Term, just like IndexWriter. But IndexReader returns the number of documents deleted; IndexWriter doesn’t. This is due to a difference in the implementation: IndexReader determines immediately which documents were deleted, and is therefore able to count up the affected documents; IndexWriter simply buffers the deleted Term and applies it later.
				?? IndexReader’s deletions take effect immediately, if you use that same reader for searching. This means you can do deletion and immediately run a search, and the deleted documents will no longer appear in the search results. With Index- Writer, the deletions aren’t visible until you open a new reader.
				?? IndexWriter is able to delete by Query, but IndexReader isn’t (though it’s not hard to run your own Query and simply delete every document number that was returned).
				?? IndexReader exposes a sometimes useful method, undeleteAll, which as you might infer reverses all pending deletions in the index. Note that this only reverses deletions that haven’t been merged yet. This is possible because Index- Writer simply marks the document as deleted, but does not in fact remove the document from the index until the segment containing the document is  merged, as described in the next section.
			If you’re tempted to use IndexReader for deletion, remember that Lucene allows only one “writer” to be open at once. Confusingly, an IndexReader that’s performing deletions counts as a “writer.” This means you are forced to close any open IndexWriter before doing deletions with IndexReader, and vice versa. If you find that you are quickly interleaving added and deleted documents, this will slow down your indexing throughput substantially. It’s better to batch up your additions and deletions, using IndexWriter, to get better performance.
		Reclaiming disk space used by deleted documents
			Lucene uses a simple approach to record deleted documents in the index: the document is marked as deleted in a bit array, which is a quick operation, but the data corresponding to that document still consumes disk space in the index. This technique is necessary because in an inverted index, a given document’s terms are scattered all over the place, and it’d be impractical to try to reclaim that space when the document is deleted. It’s not until segments are merged, either by normal merging over time or by an explicit call to optimize, that these bytes are reclaimed.
			You can also call expungeDeletes to explicitly reclaim all disk space consumed by deleted documents. This call merges any segments that have pending deletions. Although this will generally be a lower-cost operation than optimizing, it’s still quite costly and is likely only worthwhile when you know you’ve finished doing deletions for quite a while. In the worst case, if your deletions are scattered all over the segments so that all segments have deletions, then expungeDeletes does exactly the same thing as optimize: it merges all segments down to one. Let’s see next how IndexWriter chooses to make a new segment.
		Buffering and flushing
			IndexWriter triggers a flush according to three possible criteria, which are controlled by the application:
				?? To flush when the buffer has consumed more than a preset amount of RAM, use setRAMBufferSizeMB. The RAM buffer size shouldn’t be taken as an exact maximum of memory usage because you should consider many other factors when measuring overall JVM memory usage. Furthermore, IndexWriter doesn’t account for all of its RAM usage, such as the memory required by segment merging. Section 11.3.3 describes ideas to minimize overall JVM memory usage.
				?? It’s also possible to flush after a specific number of documents have been added by calling setMaxBufferedDocs.
				?? You can trigger flushing whenever the total number of buffered deleted terms and queries exceeds a specified count by calling setMaxBufferedDeleteTerms.
			Flushing happens whenever one of these triggers is hit, whichever comes first. There’s a constant IndexWriter.DISABLE_ AUTO_FLUSH, which you can pass to any of these methods to prevent flushing by that criterion. By default, IndexWriter flushes only when RAM usage is 16 MB.
			When a flush occurs, the writer creates new segment and deletion files in the Directory. However, these files are neither visible nor usable to a newly opened IndexReader until the writer commits the changes and the reader is reopened. It’s important to understand this difference. Flushing is done to free up memory consumed by buffered changes to the index. Committing is done to make all changes (buffered or already flushed) persistent and visible in the index. This means IndexReader always sees the starting state of the index (when IndexWriter was opened), until the writer commits.
			While an IndexWriter is making changes to the index, a newly opened IndexReader won’t see any of these changes until commit() or close() is called and the reader is reopened. This even applies to opening a new IndexWriter with create=true. But a newly opened near-real-time reader (see section 2.8) is able to see the changes without requiring a commit() or close().
		Index commits	
			A new index commit is created whenever you invoke one of IndexWriter’s commit methods. There are two such methods: commit() creates a new index commit, and commit(Map<String,String> commitUserData) records the provided string map as opaque metadata into the commit for later retrieval. Closing the writer also calls commit(). Note that a newly opened or reopened IndexReader or IndexSearcher will only see the index as of the last commit, and all changes made by IndexWriter in between calls to commit are invisible to readers. The one exception to this is the near-real-time search functionality, covered in section 2.8, which is able to search changes made with the IndexWriter without first committing those changes to disk.
			Here are the steps IndexWriter takes during commit:
				1 Flush any buffered documents and deletions.
				2 Sync all newly created files, including newly flushed files as well as any files produced by merges that have finished since commit was last called or since the IndexWriter was opened. IndexWriter calls Directory.sync to achieve this, which doesn’t return until all pending writes in the specified file have been written to stable storage on the underlying I/O system. This is usually a costly operation as it forces the OS to flush any pending writes. Different file systems also show wide variance in the cost of this operation.
				3 Write and sync the next segments_N file. Once this completes, IndexReaders will suddenly see all changes done since the last commit. 
				4 Remove old commits by calling on IndexDeletionPolicy to remove old commits. You can create your own implementation of this class to customize which commits are deleted, and when.
			Because old index files referenced only by the last commit aren’t removed until a new commit is completed, waiting a long time between commits will necessarily consume more disk space than performing more frequent commits.
			TWO-PHASE COMMIT
				For applications that need to commit a transaction involving a Lucene index and other external resources, such as a database, Lucene exposes the prepareCommit() and prepareCommit(Map<String,String> commitUserData) methods. Each method does steps 1 and 2 in our list, as well as most of step 3, but it stops short of making the new segments_N file visible to a reader. After prepareCommit() is called, you should either call rollback() to abort the commit or commit() to complete it. Commit() is a fast call if prepareCommit() was already called. If an error will be hit, such as “disk full,” most likely prepareCommit() will hit the error, not commit(). The separation of these two phases of committing allows you to build a distributed two-phase commit protocol involving Lucene.
				By default, after creating a new commit, IndexWriter removes all prior commits. But you can override this behavior by creating a custom IndexDeletionPolicy.
			INDEXDELETIONPOLICY
				IndexDeletionPolicy is the class that tells IndexWriter when it’s safe to remove old commits. The default policy is KeepOnlyLastCommitDeletionPolicy, which always removes all prior commits whenever a new commit is complete. Most of the time you should use this default. But for some advanced applications where you’d like to keep an old point-in-time snapshot around even though further changes have been committed to the index, you may implement your own policy.
			MANAGING MULTIPLE INDEX COMMITS
				Normally, a Lucene index will have only a single commit present, which is the last commit. But by implementing a custom deletion policy, you can easily accumulate many commits in the index. You can use the static IndexReader.listCommits() method to retrieve all commits present in an index. Then, you can step through each and gather whatever details you need.
				Once you’ve found a commit, you can open an IndexReader on it: several of the static open methods accept an IndexCommit. You could use this to explicitly search a previous version of the index.
				Using the same logic, you can also open an IndexWriter on a prior commit, but the use case is different: it allows you to roll back to a previous commit and start indexing new documents from that point, effectively undoing all changes to the index that had happened after that commit. This is similar to IndexWriter’s rollback method, except that method only rolls back changes done within the current IndexWriter session, whereas opening on a prior commit lets you roll back changes that were already committed to the index, perhaps long ago.
		ACID transactions and index consistency
			Lucene implements the ACID transactional model, with the restriction that only one transaction (writer) may be open at once. Here’s what ACID stands for, along with details about how Lucene meets it:
				?? Atomic—All changes done with the writer are either committed to the index, or none are; there is nothing in-between.
				?? Consistency—The index will also be consistent; for example, you’ll never see a delete without the corresponding addDocument from updateDocument; you’ll always see all or none of the indexes added from an addIndexes call.
				?? Isolation—While you are making changes with IndexWriter, no changes are visible to a newly opened IndexReader until you successfully commit. This even includes passing create=true to a newly opened IndexWriter. The IndexReader only sees the last successful commit.
				?? Durability—If your application hits an unhandled exception, the JVM crashes, the OS crashes, or the computer suddenly loses power, the index will remain consistent and will contain all changes included in the last successful commit. Changes done after that will be lost. Note that if your hard drive develops errors, or your RAM or CPU flips bits, that can easily corrupt your index.
			If your application, the JVM, the OS, or the machine crashes, the index won’t be corrupted and will automatically roll back to the last successful commit. But Lucene relies on the OS and I/O system that holds the index to properly implement the fsync system call by flushing any OS or I/O write caches to the actual underlying stable storage. In some cases, it may be necessary to disable write caching on the underlying I/O devices.
		Merging
			Merging has a couple of important benefits:
				?? It reduces the number of segments in the index because once the merge completes, all of the old segments are removed and a single large segment is added in their place. This makes searching faster since there are fewer segments to search, and also prevents hitting the file descriptor limit enforced by the OS.
				?? It reduces the size of the index. For example, if there were deletes pending on the merged segments, the merging process frees up the bytes consumed by deleted documents. Even if there are no pending deletions, a single merged segment will generally use fewer bytes to represent exactly the same set of indexed documents.
			MergePolicy only decides which merges should be done; it’s up to MergeScheduler to carry out these merges.
			MERGEPOLICY
				IndexWriter relies on a subclass of the abstract MergePolicy base class to decide when a merge should be done. Whenever new segments are flushed, or a previously selected merge has completed, the MergePolicy is consulted to determine if a merge is now necessary, and if so, precisely which segments will be merged. Besides picking “normal” segment merges to do, the MergePolicy also selects merges necessary to optimize the index and to run expungeDeletes.
				Lucene provides two core merge policies, both subclassing from LogMergePolicy. The first, which is the default used by IndexWriter, is LogByteSizeMergePolicy. This policy measures the size of a segment as the total size in bytes of all files for that segment. The second one, LogDocMergePolicy, makes the same merging decisions except it measures size of a segment by the document count of the segment. Note that neither merge policy takes deletions into account. If you have mixed document sizes, it’s best to use LogByteSizeMergePolicy because it’s a more accurate measure of segment size.
				To understand these parameters we first must understand how both of these policies select merges. For each segment, its level is computed using this formula: (int) log(max(minMergeMB, size))/log(mergeFactor)
				This effectively groups the segments of roughly equal size (in log space) into the same level. Tiny segments, less than minMergeMB, are always forced into the lowest level to prevent too many tiny segments in the index. Each level contains segments that are up to mergeFactor times larger than the previous level. For example, when using LogByte- SizeMergePolicy, level 0 segments are up to mergeFactor bytes in size; level 1 segments are up to mergeFactor^2 bytes in size, level 2 segments are up to mergeFactor^3 bytes in size, and so on. When using LogDocMergePolicy, the same progression holds but the size is measured as number of documents in each segment, not byte size.
				Once a given level has mergeFactor or more segments, they are merged. Thus, mergeFactor controls not only how segments are assigned to levels based on size, and thus when to trigger a merge, but also how many segments are merged at once. The larger this setting is, the more segments will exist in your index and the less frequently merges will be done, for a given number of documents in the index. Larger values generally result in faster indexing throughput, but may result in too many open file descriptors (see section 11.3.2 for details on controlling file descriptor usage). It’s probably best to leave this at its default value (10) unless you see strong gains when testing different values. When the merge completes, a new segment at the next higher level replaces the merged segments. To prevent merges of large segments, set max- MergeMB or maxMergeDocs. If ever a segment is over maxMergeMB in byte size, or max- MergeDocs in its document count, that segment will never be merged. By setting maxMergeDocs you can force extremely large segments to remain separate forever in your index.
				Besides selecting merges for normal ongoing maintenance of the index, Merge- Policy is responsible for selecting merges when optimize or expungeDeletes is called. In fact, it’s up to the MergePolicy to define what these methods mean.
				Over time, LogByteSizeMergePolicy produces an index with a logarithmic staircase structure: you have a few very large segments, a few segments mergeFactor smaller, and so on. The number of segments in your index is proportional to the logarithm of the net size, in bytes or number of documents, of your index. This generally does a good job of keeping segment count low while minimizing the net merge cost. But some of these settings can be tuned to improve indexing throughput
			MERGESCHEDULER
				Index-Writer relies on a subclass of MergeScheduler to achieve this. By default, IndexWriter uses ConcurrentMergeScheduler, which merges segments using background threads. There’s also SerialMergeScheduler, which always merges segments using the caller’s thread, which means you could suddenly see methods like addDocument and deleteDocuments take a long time while it executes a merge.
				If for some reason you need to wait for all merges to finish, call IndexWriter’s waitForMerges method.
	Summary
		
			
Adding Search to your Application
	Lucene’s primary searching API:
		Class 							Purpose
		IndexSearcher 					Gateway to searching an index. All searches come through an IndexSearcher instance using any of the several overloaded search methods. 
		Query (and subclasses)			Concrete subclasses encapsulate logic for a particular query type. Instances of Query are passed to an IndexSearcher’s search method.
		QueryParser 					Processes a human-entered (and readable) expression into a concrete Query object.
		TopDocs 						Holds the top scoring documents, returned by IndexSearcher.search.
		ScoreDoc 						Provides access to each search result in TopDocs.
	When you’re querying a Lucene index, a TopDocs instance, containing an ordered array of ScoreDoc, is returned. The array is ordered by score by default. Lucene computes a score (a numeric value of relevance) for each document, given a query. The ScoreDocs themselves aren’t the actual matching documents, but rather references, via an integer document ID, to the documents matched. In most applications that display search results, users access only the first few documents, so it isn’t necessary to retrieve the full document for all results; you need to retrieve for the current page only the documents that will be presented to the user. In fact, for very large indexes, it often wouldn’t even be possible, or would take far too long, to collect all matching documents into available physical computer memory.
	Implementing a simple search feature
		Searching for a specific term
			The original text may have been normalized into terms by the analyzer, which may eliminate terms (such as stop words), convert terms to lowercase, convert terms to base word forms (stemming), or insert additional terms (synonym processing). It’s crucial that the terms passed to Index- Searcher be consistent with the terms produced by analysis of the source documents during indexing.
			Note that we close the searcher, and then the directory, after we are done. In general it’s best to keep these open and share a single searcher for all queries that need to run. Opening a new searcher can be a costly operation because it must load and populate internal data structures from the index.
		Parsing an user-entered query expression: QueryParser
			Lucene includes an interesting built-in feature that parses query expressions, available through the QueryParser class. It parses rich expressions such as the two shown ("+JUNIT +ANT -MOCK" and "mock OR junit") into one of the Query implementations. The resulting Query instances can be very rich and complex!
			QueryParser is the only searching piece that uses an analyzer. Querying through the API using TermQuery and the others discussed in section 3.4 doesn’t require an analyzer but does rely on matching terms to what was indexed. Therefore, if you construct queries entirely programmatically you must ensure the terms included in all of your queries match the tokens produced by the analyzer used during indexing.
			USING QUERYPARSER
				QueryParser is instantiated with match- Version (Version), a field name (String), and an analyzer, which it uses to break the incoming search text into Terms
				The field name is the default field against which all terms will be searched, unless the search text explicitly requests matches against a different field name using the syntax “field:text”
				The parse() method is quick and convenient to use, but it may not be sufficient. There are various settings that can be controlled on a QueryParser instance, such as the default operator when multiple terms are used (which defaults to OR). These settings also include locale (for date parsing), default phrase slop (described insection 3.4.6), the minimum similarity and prefix length for fuzzy queries, the date resolution, whether to lowercase wildcard queries, and various other advanced settings.
			HANDLING BASIC QUERY EXPRESSION WITH QUERYPARSER
				Expression examples that QueryParser handles:
					Query expression 														Matches documents that…
					java 																	Contain the term java in the default field
					java junit
					java OR junit 															Contain the term java or junit, or both, in the default fielda
					+java +junit
					java AND junit 															Contain both java and junit in the default field
					title:ant 																Contain the term ant in the title field
					title:extreme–subject:sports
					title:extreme AND NOT subject:sports 									Contain extreme in the title field and don’t have sports in the subject field
					(agile OR extreme) AND methodology 										Contain methodology and must also contain agile and/or extreme, all in the default field
					title:"junit in action" 												Contain the exact phrase “junit in action” in the title field
					title:"junit action"~5 													Contain the terms junit and action within five positions of one another, in the title field
					java* 																	Contain terms that begin with java, like javaspaces, javaserver, java.net, and the exact tem java itself.
					java~ 																	Contain terms that are close to the word java, such as lava
					lastmodified: [1/1/09 TO 12/31/09] 										Have lastmodified field values between the dates January 1, 2009 and December 31, 2009
	Using IndexSearcher
		Creating an IndexSearcher
			IndexReader uses that API to interact with the index files stored during indexing, and exposes the low-level API that IndexSearcher uses for searching.
			Note that it’s IndexReader that does all the heavy lifting to open all index files and expose a low-level reader API, while Index- Searcher is a rather thin veneer. Because it’s costly to open an IndexReader, it’s best to reuse a single instance for all of your searches, and open a new one only when necessary.
			IndexReader always searches a point-in-time snapshot of the index as it existed when the IndexReader was created. If you need to search changes to the index, you’ll have to open a new reader. Fortunately, the IndexReader.reopen method is a resource-efficient means of obtaining a new IndexReader that covers all changes to the index but shares resources with the current reader when possible. Use it like this:
				IndexReader newReader = reader.reopen();
				if (reader != newReader) {
				reader.close();
				reader = newReader;
				searcher = new IndexSearcher(reader);
				}
			An IndexSearcher instance searches only the index as it existed at the time the IndexSearcher was instantiated. If indexing is occurring concurrently with searching, newer documents indexed won’t be visible to searches. In order to see the new documents, you should open a new reader.
		Performing Searches
			Once you have an IndexSearcher, simply call one of its search methods to perform a search. Under the hood, the search method does a tremendous amount of work, very quickly. It visits every single document that’s a candidate for matching the search, only accepting the ones that pass every constraint on the query. Finally, it gathers the top results and returns them to you.
		Working with TopDocs
			Primary IndexSearcher search methods:
				IndexSearcher.search method signature 													When to use
				TopDocs search(Query query, int n) 														Straightforward searches. The int n parameter specifies how many top-scoring documents to return.
				TopDocs search(Query query, Filter filter, int n) 										Searches constrained to a subset of available documents, based on filter criteria.
				TopFieldDocs search(Query query, Filter filter, int n, Sort sort) 						Searches constrained to a subset of available documents based on filter criteria, and sorted by a custom Sort object
				void search(Query query, Collector results) 											Used when you have custom logic to implement for each document visited, or you’d like to collect a different subset of documents than the top N by the sort criteria.
				void search(Query query, Filter filter, Collector results) 								Same as previous, except documents are only accepted if they pass the filter criteria.
		Paging through results
			You can choose from a couple of implementation approaches:
				?? Gather multiple pages’ worth of results on the initial search and keep the resulting ScoreDocs and IndexSearcher instances available while the user is navigating the search results.
				?? Requery each time the user navigates to a new page.
			Requerying is most often the better solution. Requerying eliminates the need to store per-user state, which in a web application can be costly, especially with a large number of users. Requerying at first glance seems a waste, but Lucene’s blazing speed more than compensates. Also, thanks to the I/O caching in modern operating systems, requerying will typically be fast because the necessary bits from disk will already be cached in RAM. Frequently users don’t click past the first page of results anyway.
			In order to requery, the original search is reexecuted, with a larger number of requested matches, and the results are displayed beginning on the desired page. How the original query is kept depends on your application architecture.
			Don’t prematurely optimize your paging implementations with caching or persistence. First implement your paging feature with a straightforward requery approach; chances are you’ll find this sufficient for your needs.
		Near-real-time search
			In the past, without this feature, you’d have to call commit on the writer, and then reopen on your reader, but this can be time consuming since commit must sync all new files in the index, an operation that’s often costly on certain operating systems and file systems because it usually means the underlying I/O device must physically write all buffered bytes to stable storage. Near-real-time search enables you to search segments that are newly created but not yet committed.
			IndexReader reader = writer.getReader(); IndexSearcher searcher = new IndexSearcher(reader);
			The important method is IndexWriter.getReader. This method flushes any buffered changes to the directory, and then creates a new IndexReader that includes the changes. If further changes are made through the IndexWriter, you use the reopen method in the IndexReader to get a new reader. If there are changes, a new reader is returned, and you should then close the old reader. The reopen method is very efficient: for any unchanged parts of the index, it shares the open files and caches with the previous reader. Only newly created files since the last open or reopen will be opened. This results in very fast, often subsecond, turnaround.
	Understanding Lucene scoring
		How Lucene scores
			It’s called the similarity scoring formula because its purpose is to measure the similarity between a query and each document that matches the query. The score is computed for each document (d) matching each term (t) in a query (q).
				Σt in q (t f (t in d ) idf (t) boost(t.field in d ) lengthNorm(t.field in d )) coord(q,d ) queryNorm(q)
				Factors in the scoring formula:
					Factor 									Description
					tf(t in d) 								Term frequency factor for the term (t) in the document (d)—how many times the term t occurs in the document. idf(t) Inverse document frequency of the term: a measure of how “unique” the term is. Very common terms have a low idf; very rare terms have a high idf.
					boost(t.field in d) 					Field and document boost, as set during indexing (see section 2.5). You may use this to statically boost certain fields and certain documents over others.
					lengthNorm(t.field in d) 				Normalization value of a field, given the number of terms within the field. This value is computed during indexing and stored in the index norms. Shorter fields (fewer tokens) get a bigger boost from this factor.
					coord(q, d) 							Coordination factor, based on the number of query terms the document contains. The coordination factor gives an AND-like boost to documents that contain more of the search terms than other documents.
					queryNorm(q) 							Normalization value for a query, given the sum of the squared weights of each of the query terms.
			Boost factors are built into the equation to let you affect a query or field’s influence on score. Field boosts come in explicitly in the equation as the boost(t.field in d) factor, set at indexing time. The default value of field boosts, logically, is 1.0. During indexing, a document can be assigned a boost, too. A document boost factor implicitly sets the starting field boost of all fields to the specified value. Field-specific boosts are multiplied by the starting value, giving the final value of the field boost factor. It’s possible to add the same named field to a document multiple times, and in such situations the field boost is computed as all the boosts specified for that field and document multiplied together.
			In addition to the explicit factors in this equation, other factors can be computed on a per-query basis as part of the queryNorm factor. Queries themselves can have an impact on the document score. Boosting a Query instance is sensible only in a multiple- clause query; if only a single term is used for searching, changing its boost would impact all matched documents equally. In a multiple-clause Boolean query, some documents may match one clause but not another, enabling the boost factor to discriminate between matching documents. Queries also default to a 1.0 boost factor.
			Most of these scoring formula factors are controlled and implemented as a subclass of the abstract Similarity class. DefaultSimilarity is the implementation used unless otherwise specified. More computations are performed under the covers of DefaultSimilarity
			It’s important to note that a change in index-time boosts or the Similarity methods used during indexing, such as lengthNorm, require that the index be rebuilt for all factors to be in sync.
		Using explain() to understand hit scoring
			IndexSearcher has an explain method, which requires a Query and a document ID and returns an Explanation object.
			The Explanation object internally contains all the gory details that factor into the score calculation. Each detail can be accessed individually if you like; but generally, dumping out the explanation in its entirety is desired. The .toString() method dumps a nicely formatted text representation of the Explanation.
	Lucene's diverse queries
		Searching by term: TermQuery
			Note that the value is case sensitive, so be sure to match the case of terms indexed; this may not be the exact case in the original document text, because an analyzer (see chapter 4) may have indexed things differently.
			TermQuerys are especially useful for retrieving documents by a key. If documents were indexed using Field.Index.NOT_ANALYZED, the same value can be used to retrieve these documents.
		Searching with a term range: TermRangeQuery
			Terms are ordered lexicographically (according to String.compareTo) within the index, allowing for straightforward searching of textual terms within a range as provided by Lucene’s TermRangeQuery. The beginning and ending terms may either be included or excluded. If either term is null, that end is open-ended. For example, a null lowerTerm means there is no lower bound, so all terms less than the upper term are included. Only use this query for textual ranges, such as for finding all names that begin with N through Q.
			The last two Boolean parameters to the TermRangeQuery state whether the start and end points are inclusive (true) or exclusive (false).
			However, TermRangeQuery can also accept a custom Collator, which is then used for range checking. Unfortunately, this process can be extremely slow for a large index because it requires enumerating every single term in the index to check if it’s within bounds. The CollationKeyAnalyzer, a contrib module, is one way to gain back the performance.
		Searching with a numberic range: NumberRangeQuery
			If you indexed your field with NumericField, you can efficiently search a particular range for that field using NumericRangeQuery. Under the hood, Lucene translates the requested range into the equivalent set of brackets in the previously indexed trie structure. Each bracket is a distinct term in the index whose documents are OR’d together. The number of brackets required will be relatively small, which is what gives NumericRangeQuery such good performance when compared to an equivalent TermRangeQuery.
			NumericRangeQuery also optionally accepts the same precisionStep parameter as NumericField. If you had changed this value from its default during indexing, it’s crucial that you provide an acceptable value (either the same value, or a multiple of the value used during indexing) when searching. Otherwise you’ll silently get incorrect results.
		Searching on a string: PrefixQuery
			PrefixQuery matches documents containing terms beginning with a specified string. It’s deceptively handy.
		Combining queries: BooleanQuery
			The query types discussed here can be combined in complex ways using Boolean- Query, which is a container of Boolean clauses. A clause is a subquery that can be required, optional, or prohibited. These attributes allow for logical AND, OR, and NOT combinations.
			A BooleanQuery can be a clause within another BooleanQuery, allowing for arbitrary nesting.
			BooleanQuery.add has two overloaded method signatures. One accepts only a BooleanClause, and the other accepts a Query and a BooleanClause.Occur instance. A BooleanClause is simply a container of a Query and a BooleanClause.Occur instance, so we omit coverage of it. BooleanClause.Occur.MUST means exactly that: only documents matching that clause are considered. BooleanClause.Occur.SHOULD means the term is optional. BooleanClause.Occur.MUST_NOT means any documents matching this clause are excluded from the results.
			BooleanQuerys are restricted to a maximum number of clauses; 1,024 is the default. This limitation is in place to prevent queries from accidentally adversely affecting performance. A TooManyClauses exception is thrown if the maximum is exceeded. This had been necessary in past Lucene releases, because certain queries would rewrite themselves under the hood to the equivalent BooleanQuery. But as of 2.9, these queries are now executed in a more efficient manner. Should you ever have the unusual need of increasing the number of clauses allowed, there’s a setMax- ClauseCount(int) method on BooleanQuery, but be aware of the performance cost of executing such queries.
		Searching by phrase: PhraseQuery
			An index by default contains positional information of terms, as long as you didn’t create pure Boolean fields by indexing with the omitTermFreqAndPositions option (described in section 2.4.1). PhraseQuery uses this information to locate documents where terms are within a certain distance of one another.
			The maximum allowable positional distance between terms to be considered a match is called slop. Distance is the number of positional moves of terms used to reconstruct the phrase in order.
			Phrase queries are created by adding terms in the desired order. By default, a PhraseQuery has its slop factor set to zero, specifying an exact phrase match.
			Terms added to a phrase query don’t have to be in the same order found in the field, although order does impact slop-factor considerations. For example, had the terms been reversed in the query (fox and then quick), the number of moves needed to match the document would be three, not one.
			MULTIPLE-TERM PHRASES
				PhraseQuery supports multiple-term phrases. Regardless of how many terms are used for a phrase, the slop factor is the maximum total number of moves allowed to put the terms in order.
			PHRASE QUERY SCORING
				Phrase queries are scored based on the edit distance needed to match the phrase. More exact matches count for more weight than sloppier ones.
				Terms surrounded by double quotes in QueryParser-parsed expressions are translated into a PhraseQuery. The slop factor defaults to 0, but you can adjust the slop factor by adding a tilde (~) followed by an integer. For example, the expression "quick fox"~3 is a PhraseQuery with the terms quick and fox and a slop factor of 3.
		Searching by wildcard: WildcardQuery
			Wildcard queries let you query for terms with missing pieces but still find matches. Two standard wildcard characters are used: * for zero or more characters, and ? forzero or one character.
			Note how the wildcard pattern is created as a Term (the pattern to match) even though it isn’t explicitly used as an exact term under the covers. Internally, it’s used as a pattern to match terms in the index. A Term instance is a convenient placeholder to represent a field name and an arbitrary string.
			Performance degradations can occur when you use WildcardQuery. A larger prefix (characters before the first wildcard character) decreases the number of terms enumerated to find matches. Beginning a pattern with a wildcard query forces the term enumeration to search all terms in the index for matches.
			Oddly, the closeness of a wildcard match has no effect on scoring.
		Searching for similar terms: FuzzyQuery
			Lucene’s FuzzyQuery matches terms similar to a specified term. The Levenshtein distance algorithm determines how similar terms in the index are to a specified target term. (See http://en.wikipedia.org/wiki/Levenshtein_Distance for more information about Levenshtein distance.) Edit distance is another term for Levenshtein distance; it’s a measure of similarity between two strings, where distance is measured as the number of character deletions, insertions, or substitutions required to transform one string to the other string.
			Levenshtein distance isn’t the same as the distance calculation used in Phrase-Query and PrefixQuery. The phrase query distance is the number of term moves to match, whereas Levenshtein distance is an intraterm computation of character moves.
			FuzzyQuery uses a threshold rather than a pure edit distance. The threshold is a factor of the edit distance divided by the string length. Edit distance affects scoring; terms with less edit distance are scored higher. Other term statistics, such as the inverse document frequency, are also factored in
			FuzzyQuery enumerates all terms in an index to find terms within the allowable threshold. Use this type of query sparingly or at least with the knowledge of how it works and the effect it may have on performance.
		Matching all documents: MatchAllDocsQuery
			MatchAllDocsQuery, as the name implies, simply matches every document in your index. By default, it assigns a constant score, the boost of the query (default: 1.0), to all documents that match. If you use this as your top query, it’s best to sort by a field other than the default relevance sort.
			It’s also possible to have MatchAllDocsQuery assign as document scores the boosting recorded in the index, for a specified field, like so:
				Query query = new MatchAllDocsQuery(field);
			If you do this, documents are scored according to how the specified field was boosted
	Parsing query expressions
		Whenever special characters are used in a query expression, you need to provide an escaping mechanism so that the special characters can be used in a normal fashion. QueryParser uses a backslash (\) to escape special characters within terms. The characters that require escaping are as follows: \ + - ! ( ) : ^ ] { } ~ * ?
		Query.toString()
			All concrete core Query classes we’ve discussed in this chapter have a special toString() implementation. The standard Object.toString() method is overridden and delegates to a toString(String field) method, where field is the name of the default field. Calling the no-arg toString() method uses an empty default field name, causing the output to explicitly use field selector notation for all terms.
		TermQuery
			Note how QueryParser built the term query by appending the default field we’d provided when instantiating it, subject, to the analyzed term, computers.
		Term range searches	
			Text or date range queries use bracketed syntax, with TO between the beginning term and ending term. Note that TO must be all caps. The type of bracket determines whether the range is inclusive (square brackets) or exclusive (curly braces). Note that, unlike with the programmatic construction of NumericRangeQuery or TermRange- Query, you can’t mix inclusive and exclusive: both the start and end term must be either inclusive or exclusive.
			Nondate range queries lowercase the beginning and ending terms as the user entered them, unless QueryParser.setLowercaseExpanded- Terms(false) has been called. The text isn’t analyzed. If the start or end terms contain whitespace, they must be surrounded with double quotes, or parsing fails.
		Numberic and date range searches	
			QueryParser won’t create a NumericRangeQuery for you. This is because Lucene currently doesn’t keep track of which of your fields were indexed with NumericField, though it’s possible this limitation has been corrected by the time you read this. QueryParser does include certain built-in logic for parsing dates when they appear as part of a range query, but the logic doesn’t work when you’ve indexed your dates using NumericField. Fortunately, subclassing QueryParser to correctly handle numeric fields is straightforward
		Prefix and wildcard queries
			If a term contains an asterisk or a question mark, it’s considered a WildcardQuery. When the term contains only a trailing asterisk, QueryParser optimizes it to a Prefix- Query instead. Both prefix and wildcard queries are lowercased by default, but this behavior can be controlled
			Wildcards at the beginning of a term are prohibited using QueryParser by default, which you can override at the expense of performance by calling the setAllow- LeadingWildcard method.
		Boolean Operators
			Constructing Boolean queries textually via QueryParser is done using the operators AND, OR, and NOT. Note that these operators must be typed as all caps. Terms listed without an operator specified use an implicit operator, which by default is OR. The query abc xyz will be interpreted as either abc OR xyz or abc AND xyz, based on the implicit operator setting. To switch parsing to use AND: parser.setDefaultOperator(QueryParser.AND_OPERATOR);
			Placing a NOT in front of a term excludes documents matching the following term. Negating a term must be combined with at least one non-negated term to return documents; in other words, it isn’t possible to use a query like NOT term to find all documents that don’t contain a term. Each of the uppercase word operators has shortcut syntax
		Phrase Queries
			Terms enclosed in double quotes create a PhraseQuery. The text between the quotes is analyzed; thus the resultant PhraseQuery may not be exactly the phrase originally specified. This process has been the subject of some confusion. For example, the query "This is Some Phrase*", when analyzed by the StandardAnalyzer, parses to a PhraseQuery using the phrase “some phrase.” The StandardAnalyzer removes the words this and is because they match the default stop word list and leaves positionalholes recording that the words were removed (more in section 4.3.2 on Standard- Analyzer). A common question is why the asterisk isn’t interpreted as a wildcard query. Keep in mind that surrounding text with double quotes causes the surrounded text to be analyzed and converted into a PhraseQuery. Single-term phrases are optimized to a TermQuery.
			The default slop factor is 0, but you can change this default by calling QueryParser.setPhraseSlop. The slop factor can also be overridden for each phrase by using a trailing tilde (~) and the desired integer slop value
		Fuzzy queries
			A trailing tilde (~) creates a fuzzy query on the preceding term. Note that the tilde is also used to specify sloppy phrase queries, but the context is different. Double quotes denote a phrase query and aren’t used for fuzzy queries. You can optionally specify a trailing floating point number to specify the minimum required similarity.
		MatchAllDocsQuery
			QueryParser produces the MatchAllDocsQuery when you enter *:*.
		Grouping
		Field selection
			As you’ve seen, the default field name is provided when you create the QueryParser instance. Parsed queries aren’t restricted, however, to searching only the default field. Using field selector notation, you can specify terms in nondefault fields.
		Setting the boost for a subquery
			A caret (^) followed by a floating-point number sets the boost factor for the preceding query. For example, the query expression junit^2.0 testing sets the junit Term- Query to a boost of 2.0 and leaves the testing TermQuery at the default boost of 1.0. You can apply a boost to any type of query, including parenthetical groups.
		QueryParse or not to QueryParse?
			You can often obtain a happy medium by combining a QueryParser-parsed query with API-created queries as clauses in a BooleanQuery.
	Summary
		

Lucene's analysis process
	An analyzer is an encapsulation of the analysis process. An analyzer tokenizes text by performing any number of operations on it, which could include extracting words, discarding punctuation, removing accents from characters, lowercasing (also called normalizing), removing common words, reducing words to a root form (stemming), or changing words into the basic form (lemmatization). This process is also called tokenization, and the chunks of text pulled from a stream of text are called tokens. Tokens, combined with their associated field name, are terms.
	Using Analyzers
		If you highlight hits in your search results (which we strongly recommend because it gives a better end-user experience), you may need to analyze text at that point as well.
		Only the tokens produced by the analyzer are searchable, unless the field is indexed with Field.Index.NOT_ANALYZED or Field.Index.NOT_ ANALYZED_NO_NORMS, in which case the entire field’s value, as a single token, is searchable.
		In the meantime, here’s a summary of each of these analyzers:
			?? WhitespaceAnalyzer, as the name implies, splits text into tokens on whitespace characters and makes no other effort to normalize the tokens. It doesn’t lowercase each token.
			?? SimpleAnalyzer first splits tokens at nonletter characters, then lowercases eachtoken. Be careful! This analyzer quietly discards numeric characters but keeps all other characters.
			?? StopAnalyzer is the same as SimpleAnalyzer, except it removes common words. By default, it removes common words specific to the English language (the, a, etc.), though you can pass in your own set.
			?? StandardAnalyzer is Lucene’s most sophisticated core analyzer. It has quite a bit of logic to identify certain kinds of tokens, such as company names, email addresses, and hostnames. It also lowercases each token and removes stop words and punctuation.
		Indexing Analysis
			both the addDocument and updateDocument methods in IndexWriter optionally accept an analyzer to be used for that one document.
			To index the entire field’s value as a single token, like Field 3 in figure 4.1, pass Field.Index.NOT_ANALYZED or Field.Index.NOT_ANALYZED_NO_NORMS as the fourth argument. One example where this is usually required is if you intend to sort on the field,
			new Field(String, String, Field.Store.YES, Field.Index.ANALYZED) creates a tokenized and stored field. Rest assured the original String value is stored. But the output of the designated Analyzer dictates what’s indexed and available for searching.
		QueryParser Analysis
			Should you use the same analyzer with QueryParser that you used during indexing? It depends. If you stick with the basic built-in analyzers, you’ll probably be fine using the same analyzer in both situations. But when you’re using more sophisticated analyzers, quirky cases can come up in which using different analyzers between indexing and QueryParser is necessary.
		Parsing vs Analysis: when an analysis isn't appropriate
			Analyzers don’t help in field separation because their scope is to deal with a single field at a time.
	What's inside an analyzer
		The Analyzer class is the abstract base class. Quite elegantly, it turns text into a stream of tokens enumerated by the TokenStream class. The single required method signature implemented by analyzers is public TokenStream tokenStream(String fieldName, Reader reader) The returned TokenStream is then used to iterate through all tokens.
		The reusableTokenStream method is an additional, optional method that an analyzer can implement to gain better indexing performance. That method is allowed to reuse the same TokenStream that it had previously returned to the same thread. Thisapproach can save a lot of allocation and garbage collection cost because every field of every document otherwise needs a new TokenStream. Two utility methods are implemented in the Analyzer base class, setPreviousTokenStream and getPrevious- TokenStream, to store and retrieve a TokenStream in thread local storage. All the builtin Lucene analyzers implement this method: the first time the method is called from a given thread, a new TokenStream instance is created and saved. Subsequent calls return the previous TokenStream after resetting it to the new Reader.
		What's in a token
			A stream of tokens is the fundamental output of the analysis process.
			A token carries with it a text value (the word itself) as well as some metadata: the start and end character offsets in the original text, a token type, and a position increment. The token may also optionally contain application defined bit flags and an arbitrary byte[] payload, and can be easily extended to include any application specific attributes.
			The start offset is the character position in the original text where the token text begins, and the end offset is the position just after the last character of the token text. These offsets are useful for highlighting matched tokens in search results, as described in chapter 8. The token type is a String, defaulting to "word", that you can control and use in the token-filtering process if desired. As text is tokenized, the position relative to the previous token is recorded as the position increment value. Most of the built-in tokenizers leave the position increment at the default value of 1, indicating that all tokens are in successive positions, one after the other. Each token also has optional flags; a flag is a set of 32 bits (stored in an int) that’s unused by Lucene’s built-in analyzers but could be used by your application. Likewise, each token can have a byte[] recorded in the index, referred to as the payload.
			TOKENS INTO TERMS
				After text is analyzed during indexing, each token is posted to the index as a term. The position increment, start, and end offsets and payload are the only additional metadata associated with the token that’s recorded in the index. The token type and flags are discarded—they’re only used during the analysis process.
			POSITION INCREMENTS
				The token position increment value relates the current token’s position to the previous token’s position. Position increment is usually 1, indicating that each word is in a unique and successive position in the field. Position increments factor directly into performing phrase queries (see section 3.4.6) and span queries (see section 5.5), which rely on knowing how far terms are from one another within a field. Position increments greater than 1 allow for gaps and can be used to indicate where words have been removed. See section 4.6.1 for an example, where stop-word removal leaves gaps using position increments. A token with a zero position increment places the token in the same position as the previous token. Analyzers that inject synonyms can use a position increment of zero for the synonyms. The effect is that phrase queries work regardless of which synonym was used in the query.
		TokenStream uncensored
			A TokenStream is a class that can produce a series of tokens when requested, but there are two very different styles of TokenStreams: Tokenizer and TokenFilter. They both inherit from the abstract TokenStream class, as shown in figure 4.3. Note the composite pattern used by TokenFilter to encapsulate another TokenStream (which could, of course, be another TokenFilter). A Tokenizer reads characters from a java. io.Reader and creates tokens, whereas a TokenFilter takes tokens in, and produces new tokens by either adding or removing whole tokens or altering the attributes of the incoming tokens.
			When an analyzer returns a TokenStream from its tokenStream or reusableTokenStream method, it typically starts with a single Tokenizer, which creates the initial sequence of tokens, then chains together any number of TokenFilters to modify these tokens. This is referred to as the analyzer chain.
			An analyzer chain starts with a Tokenizer, to produce initial tokens from the characters read from a Reader, then modifies the tokens with any number of chained TokenFilters.
			Analyzer building blocks provided in Lucene’s core API:
			Class name 							Description
			TokenStream 						Abstract Tokenizer base class.
			Tokenizer 							TokenStream whose input is a Reader.
			CharTokenizer 						Parent class of character-based tokenizers, with abstract isTokenChar() method. Emits tokens for contiguous blocks when isTokenChar() returns true. Also provides the capability to normalize (for example, lowercase) characters. Tokens are limited to a maximum size of 255 characters.
			WhitespaceTokenizer 				CharTokenizer with isTokenChar() true for all nonwhitespace characters.
			KeywordTokenizer 					Tokenizes the entire input string as a single token.
			LetterTokenizer 					CharTokenizer with isTokenChar() true when Character.isLetter is true.
			LowerCaseTokenizer 					LetterTokenizer that normalizes all characters to lowercase. 
			SinkTokenizer 						A Tokenizer that absorbs tokens, caches them in a private list, and can later iterate over the tokens it had previously cached. This is used in conjunction with TeeTokenizer to “split” a TokenStream.
			StandardTokenizer 					Sophisticated grammar-based tokenizer, emitting tokens for high-level types like email addresses (see section 4.3.2 for more details). Each emitted token is tagged with a special type, some of which are handled specially by StandardFilter.
			TokenFilter 						TokenStream whose input is another TokenStream.
			LowerCaseFilter 					Lowercases token text.
			StopFilter 							Removes words that exist in a provided set of words.
			PorterStemFilter 					Stems each token using the Porter stemming algorithm. For example, country and countries both stem to countri.
			TeeTokenFilter 						Splits a TokenStream by passing each token it iterates through into a SinkTokenizer. It also returns the token unnmodified to its caller.
			ASCIIFoldingFilter 					Maps accented characters to their unaccented counterparts. CachingTokenFilter Saves all tokens from the input stream and can replay the stream once reset is called.
			LengthFilter 						Accepts tokens whose text length falls within a specified range.
			StandardFilter 						Designed to be fed by a StandardTokenizer. Removes dots from acronyms and ’s (apostrophe followed by s) from words with apostrophes.
			Buffering is a feature that’s commonly needed in the TokenStream implementations. Low-level Tokenizers do this to buffer up characters to form tokens at boundaries such as whitespace or nonletter characters. TokenFilters that emit additional tokens into the stream they’re filtering must queue an incoming token and the additional ones and emit them one at a time; our SynonymFilter in section 4.5 is an example of such a filter.
			Most of the built-in TokenFilters alter a single stream of input tokens in some fashion, but one of them, TeeSinkTokenFilter, is more interesting. This is a filter that clones an incoming token stream into any number of output streams called sinks. It reads tokens from its single input source, then sends a copy of that token to all of its sink output streams as well as its output stream. Each of the sink streams can undergo its own further processing. This is useful when two or more fields would like to share the same initial analysis steps but differ on the final processing of the tokens.
		Visualizing analyzers
			LOOKING INSIDE TOKENS
				TokenFilters access and alter the attributes of tokens that flow through them.
				TokenStream stream = analyzer.tokenStream("contents", new StringReader(text));
				TermAttribute term = stream.addAttribute(TermAttribute.class);
				PositionIncrementAttribute posIncr = stream.addAttribute(PositionIncrementAttribute.class);
				OffsetAttribute offset = stream.addAttribute(OffsetAttribute.class);
				TypeAttribute type = stream.addAttribute(TypeAttribute.class);
				int position = 0;
				while(stream.incrementToken()) {
					int increment = posIncr.getPositionIncrement();
					if (increment > 0) {
						position = position + increment;
						System.out.println();
						System.out.print(position + ": ");
					}
					System.out.print("[" +
						term.term() + ":" +
						offset.startOffset() + "->" +
						offset.endOffset() + ":" +
						type.type() + "] ");
				}
				System.out.println();
			ATTRIBUTES
				Notice that the TokenStream never explicitly creates a single object holding all attributes for the token. Instead, you interact with a separate reused attribute interface for each element of the token (term, offsets, position increments, etc.).
				TokenStream subclasses from a class called AttributeSource (in org.apache. lucene.util). AttributeSource is a useful and generic means of providing strongly typed yet fully extensible attributes without requiring runtime casting, thus resulting in good performance.
				Note that Lucene will do nothing with your new attribute during indexing, so this is only currently useful in cases where one TokenStream early in your analysis chain wishes to send information to another TokenStream later in the chain.
				Lucene’s built-in token attributes:
					Token attribute interface 			Description
					TermAttribute 						Token’s text
					PositionIncrementAttribute 			Position increment (defaults to 1)
					OffsetAttribute 					Start and end character offset
					TypeAttribute 						Token’s type (defaults to word)
					FlagsAttribute 						Bits to encode custom flags
					PayloadAttribute 					Per-token byte[] payload (see section 6.5)
				With this reusable API, you first obtain the attributes of interest by calling the add- Attribute method, which will return a concrete class implementing the requested interface. Then, you iterate through all tokens by calling TokenStream.increment- Token. This method returns true if it has advanced to a new token and false once you’ve exhausted the stream. You then interact with the previously obtained attributes to get that attribute’s value for each token. When incrementToken returns true, all attributes within it will have altered their internal state to the next token.
				Note that the core attribute classes in table 4.2 are bidirectional: you can use them to get and set the value for that attribute. Thus, a TokenFilter that alters only the position increment would grab and store the PositionIncrementAttribute from its input TokenStream when it’s first instantiated, then implement the incrementToken method by first calling incrementToken on its input stream and calling PositionIncrement- Attribute.setPositionIncrement to change the value.
				Sometimes you need to take a complete copy of all details for the current token and restore it later. You can do this by calling captureState, which returns a State object holding all state. You can later restore that state by calling restoreState. Note that this results in slower performance so you should avoid doing so, if possible, when creating your own TokenFilters.
			WHAT GOOD ARE START AND END OFFSETS?
				The start and end offset values, which record the original character offset at the start and end of each token’s text, aren’t used in the core of Lucene. Rather, they’re treated as opaque integers for each token, and you could put any arbitrary integers you’d like into there.
				If you index with TermVectors, as described in section 2.4.3, and specify that the offsets are stored, then at search time you can retrieve the TermVectors for a given document and access the offsets. Often this is used for highlighting, as discussed in chapter 8. It’s also possible to reanalyze the text to do highlighting without storing TermVectors, in which case the start and end offsets are recomputed by the analyzer, then used in real time.
			TOKEN TYPE USEFULLNESS
				By default, Lucene doesn’t record the token type into the index; thus, it only serves a purpose during analysis. But you can use the TypeAsPayloadTokenFilter to record the type of each token as a payload.
				You can use the token type to denote special lexical types for tokens. Under the covers of StandardAnalyzer is a StandardTokenizer that parses the incoming text into different types based on a grammar. Analyzing the phrase “I’ll email you at xyz@example. com” with StandardAnalyzer produces this interesting output:
					1: [i'll:0->4:<APOSTROPHE>]
					2: [email:5->10:<ALPHANUM>]
					3: [you:11->14:<ALPHANUM>]
					5: [xyz@example.com:18->33:<EMAIL>]
		TokenFilter order can be significant
			There may also be performance considerations when you order the filtering process. Consider an analyzer that removes stop words and injects synonyms into the token stream—it would be more efficient to remove the stop words first sothat the synonym injection filter would have fewer terms to consider
			An analyzer simply defines a specific chain of tokenizers, beginning with an original source of new tokens (TokenStream) followed by any number of TokenFilters that alter the tokens. A Token consists of values for a certain set of interesting attributes, which Lucene stores in different ways.
	Using the built-in analyzers
		Primary analyzers available in Lucene:
			Analyzer 				Steps taken
			WhitespaceAnalyzer 		Splits tokens at whitespace.
			SimpleAnalyzer 			Divides text at nonletter characters and lowercases.
			StopAnalyzer 			Divides text at nonletter characters, lowercases, and removes stop words.
			KeywordAnalyzer 		Treats entire text as a single token.
			StandardAnalyzer 		Tokenizes based on a sophisticated grammar that recognizes email addresses, acronyms, Chinese-Japanese-Korean characters, alphanumerics, and more. It also lowercases and removes stop words.
		StopAnalyzer	
			Embedded in StopAnalyzer is the following set of common English stop words, defined as ENGLISH_STOP_WORDS_SET. This default set is used unless otherwise specified. The StopAnalyzer has a second constructor that allows you to pass your own set instead.
		StandardAnalyzer
			StandardAnalyzer also includes stop-word removal, using the same mechanism as the StopAnalyzer (identical default English set, and an optional Set constructor to override).
		Which core analyzer should you use?
	Sound-like querying		
	Synonyms, aliases, and words that mean the same
		Creating SynonymAnalyzer
			SynonymAnalyzer’s purpose is to first detect the occurrence of words that have synonyms, and second to insert the synonyms at the same position.
			Because synonyms are indexed just like other terms, TermQuery works as expected. Also, PhraseQuery works as expected when we use a synonym in place of an original word.
			But this is wasteful and unnecessary: we only need synonym expansion during indexing or during searching, not both. If you choose to expand during indexing, the disk space consumed by your index will be somewhat larger, but searching may be faster because there are fewer search terms to visit. Your synonyms have been baked into the index, so you don’t have the freedom to quickly change them and see the impact of such changes during searching. If instead you expand at search time, you can see fast turnaround when testing. These are simply trade-offs, and which option is best is your decision based on your application’s constraints.
		Visualizing token positions
		Stemming analysis
			PositionalPorterStopAnalyzer. This analyzer removes stop words, leaving positional holes where words are removed, and leverages a stemming filter.
			The PorterStemFilter is shown in the class hierarchy in figure 4.5, but it isn’t used by any built-in analyzer. It stems words using the Porter stemming algorithm created by Dr. Martin Porter
			In other words, the various forms of a word are reduced to a common root form. For example, the words breathe, breathes, breathing, and breathed, via the Porter stemmer, reduce to breath.
			StopFilter leaves holes
				If you have a need to disable the holes so that position increment is always 1, use StopFilter’s setEnable- PositionIncrements method. But be careful when doing so: your index won’t record the deleted words, so there can be surprising effects.
				There’s an interesting alternative, called shingles, which are compound tokens created from multiple adjacent tokens. Lucene has a TokenFilter called ShingleFilter in the contrib analyzers module that creates shingles during analysis. We’ll describe it in more detail in section 8.2.3. With shingles, stop words are combined with adjacent words to make new tokens, such as the-quick. At search time, the same expansion is used. This enables precise phrase matching, because the stop words aren’t discarded. Using shingles yields good search performance because the number of documents containing the-quick is far fewer than the number containing the stop word the in any context.
			Combining stemming and stop-word removal
	Field Variations
		Analysis of multivalued fields
			a document may have more than one Field instance with the same name, and that Lucene logically appends the tokens of these fields sequentially during indexing. Fortunately, your analyzer has some control over what happensat each field value boundary. This is important in order to ensure queries that pay attention to a Token’s position, such as phrase or span queries, don’t inadvertently match across two separate field instances.
			To fix this, you’ll have to create your own analyzer by subclassing the Analyzer class, then override the getPositionIncrementGap method (along with the token- Stream or reusableTokenStream method). By default, getPositionIncrementGap returns 0 (no gap), which means it acts as if the field values were directly appended to one another. Increase it to a large enough number (for example, 100) so that no positional queries could ever incorrectly match across the boundary.
			It’s also important to ensure that token offsets are computed properly for multivalued fields. If you intend to highlight such fields, as described in section 8.3, incorrectoffsets will cause the wrong parts of the text to be highlighted. The token’s Offset- Attribute, which exposes methods to retrieve the start and end offset, also has a special method endOffset, whose purpose is to return the final offset for the field. This is necessary for cases where a TokenFilter has stripped out one or more final tokens; Lucene would otherwise have no way to compute the final offset for that field value. The offsets of each Field instance are shifted by the sum of the endOffset of all fields before it. Lucene’s core tokenizers all implement endOffset properly, but if you create your own tokenizer, it’s up to you to do so. Similarly, if your application requires a gap to be added to offsets when a field has multiple values, you should override the getOffsetGap method of your custom analyzer.
		Field specific analysis	
			Lucene has a helpful built-in utility class, PerFieldAnalyzerWrapper, that makes it easy to use different analyzers per field. Use it like this: PerFieldAnalyzerWrapper analyzer = new PerFieldAnalyzerWrapper( new SimpleAnalyzer()); analyzer.addAnalyzer("body", new StandardAnalyzer(Version.LUCENE_30)); You provide the default analyzer when you create PerFieldAnalyzerWrapper. Then, for any field that requires a different analyzer, you call the addAnalyzer method. Any field that wasn’t assigned a specific analyzer simply falls back to the default one. In the previous example, we use SimpleAnalyzer for all fields except body, which uses StandardAnalyzer.
		Searching on unanalyzed fields
			But a dilemma can arise if you use QueryParser and attempt to query on an unanalyzed field; this is because the fact that the field wasn’t analyzed is only known during indexing. There’s nothing special about such a field’s terms once indexed; they’re just terms.
			We’ll use Lucene’s KeywordAnalyzer to tokenize the part number as a single token. Note that KeywordAnalyzer and Field.Index.NOT_ANALYZED* are identical during indexing; it’s only with QueryParser that using KeywordAnalyzer is necessary.We want only one field to be “analyzed” in this manner, so we leverage the PerField- AnalyzerWrapper to apply it only to the partnum field.
			PerFieldAnalyzerWrapper analyzer = new PerFieldAnalyzerWrapper(new SimpleAnalyzer()); analyzer.addAnalyzer("partnum", new KeywordAnalyzer()); We use PerFieldAnalyzerWrapper to apply the KeywordAnalyzer only to the partnum field, and SimpleAnalyzer to all other fields. This yields the same result as during indexing. The query now has the proper term for the partnum field, and the document is found as expected.
			Using a KeywordAnalyzer on special fields during indexing would eliminate the use of Index.NOT_ANALYZED_NO_NORMS during indexing and replace it with Index.ANALYZED. Aesthetically, it may be pleasing to see the same analyzer used during indexing and querying, and using PerFieldAnalyzerWrapper makes this possible.
			We’ve seen some interesting situations arising for different kinds of fields. Multivalued fields require setting a position increment gap, to avoid matching across different values, while PerFieldAnalyzerWrapper lets us customize which analyzer is used for which field.
	Language Analysis Issues
		Unicode and encodings
			Internally, Lucene stores all characters in the standard UTF-8 encoding. Java frees us from many struggles by automatically handling Unicode within Strings, represented as UTF16 code points, and providing facilities for reading in external data in the many encodings.
		Analyzing non-English languages
		Character normalization
			Lucene makes it possible to normalize the character stream seen by the Tokenizer. This normalization fits in between the Reader and the Tokenizer, filtering the characters produced by the Reader, as shown in figure 4.7. What’s crucial about this API is it properly accounts for the necessary corrections to the start and end offsets of Tokens whenever the filtering adds or removes characters. This means highlighting will work correctly in the original input string.
			Regardless of your reasons, Lucene provides a set of character filtering classes that mirrors their token-based counterparts. The CharStream abstract base class simplyadds one method, correctOffset, to the Reader class. CharReader wraps a normal Reader and creates a CharStream, whereas CharFilter chains any CharStream together. Using these building blocks, you can create a character filter chain, beginning with a single CharReader followed by any number of CharFilters, before tokenization even gets started.
			Lucene provides a single core concrete implementation of CharFilter, called MappingCharFilter, that allows you to enroll input and output pairs of substrings. Whenever one of the input substrings is seen in the input character stream, it’s replaced with the corresponding output string. Although you can use this class as is, ifyou want to perform simple substring replacement keep in mind that it has a potentially high performance cost. That’s because the current implementation allocates many temporary objects during analysis.
			None of the core analyzers perform character filtering. You’ll have to create your own analyzer that builds a chain starting with a CharReader followed by any number of CharFilters, then a Tokenizer and TokenFilter chain.
		Analyzing Asian languages
			The only built-in analyzer capable of doing anything useful with Asian text is the StandardAnalyzer, which recognizes some ranges of the Unicode space as CJK characters and tokenizes them individually.
		Zaijian
			When you’re indexing documents in multiple languages into a single index, using a per-document analyzer is appropriate. You may also want to add a field to documents indicating their language; this field can be used to filter search results or for display purposes during retrieval. In
	Nutch Analysis
		Nutch takes an interesting approach to analyzing text, specifically how it handles stop words, which it calls common terms. If all words are indexed, an enormous number of documents become associated with each common term, such as the. Querying for the is practically a nonsensical query, given that the majority of documents contain that term. When common terms are used in a query, but not within a phrase, such as the quick brown with no other adornments or quotes, they are discarded. However, if a series of terms is surrounded by double quotes, such as “the quick brown,” a fancier trick is played
		Nutch combines an index-time analysis bigram (grouping two consecutive words as a single token) technique with a query-time optimization of phrases. This results ina far smaller document space considered during searching; for example, far fewer documents have the quick side by side than contain the.
	Summary
		
			
Advanced search techniques
	Lucene's field cache
		Note that the field cache isn’t a user-visible search feature; rather, it’s something of a building block, a useful internal API that you can use when implementing advanced search features in your application.	
		One important restriction for using the field cache is that all documents must have a single value for the specified field. This means the field cache can’t handle multivalued fields as of Lucene 3.0, though it’s possible this restriction has been relaxed by the time you’re reading this.	
		A field cache can only be used on fields that have a single term. This typically means the field was indexed with Index.NOT_ANALYZED or Index.NOT_ANALYZED_NO_NORMS, though it’s also possible to analyze the fields as long as you’re using an analyzer, such as KeywordAnalyzer, that always produces only one token.	
		Loading fields values for all documents
			you can get the weight for all documents like this: float[] weights = FieldCache.DEFAULT.getFloats(reader, "weight"); Then, simply reference weights[docID] whenever you need to know a document’s weight value.
			The first time the field cache is accessed for a given reader and field, the values for all documents are visited and loaded into memory as a single large array, and recorded into an internal cache keyed on the reader instance and the field name. This process can be quite time consuming for a large index. Subsequent calls quickly return the same array from the cache. The cache entry isn’t cleared until the reader is closed and completely dereferenced by your application (a WeakHashMap, keyed by the reader, is used under the hood). This means that the first search that uses the field cache will pay the price of populating it. If your index is large enough that this cost is too high, it’s best to prewarm your IndexSearchers before using them for real queries
			It’s important to factor in the memory usage of field cache. Numeric fields require the number of bytes for the native type, multiplied by the number of documents. ForString types, each unique term is also cached for each document. For highly unique fields, such as title, this can be a large amount of memory, because Java’s String object itself has substantial overhead. The StringIndex field cache, which is used when sorting by a string field, also stores an additional int array holding the sort order for all documents.
			The field cache may consume quite a bit of memory; each entry allocates an array of the native type, whose length is equal to the number of documentsin the provided reader. The field cache doesn’t clear its entries until you close your reader and remove all references to that reader from your application and garbage collection runs.
		Per-segment readers
			As of 2.9, Lucene drives all search results collection and sorting one segment at a time. This means the reader argument passed to the field cache by Lucene’s core functionality will always be a reader for a single segment. This has strong benefits when reopening an IndexReader; only the new segments must be loaded into thefield cache.
			But this means you should avoid passing your top-level IndexReader to the field cache directly to load values, because you’d then have values double-loaded, thus consumingtwice as much RAM. Typically, you require the values in an advanced customization, such as implementing a custom Collector, a custom Filter, or a custom FieldComparatorSource, as described in chapter 6. All of these classes are provided with the single-segment reader, and it’s that reader that you should in turn pass to the field cache to retrieve values. If the field cache is using too much memory and yoususpect that a top-level reader may have been accidentally enrolled, try using the set- InfoStream API to enable debugging output. Cases like this one, plus other situations such as the same reader and field loaded under two different types, will cause a detailed message to be printed to the PrintStream you provide.
			Avoid passing a top-level reader directly to the field cache API. This can result in consuming double the memory, if Lucene is also passing individual segments’ readers to the API.
	Sorting search results
		Remember that sorting under the hood uses the field cache to load values across all documents, so keep the performance trade-offs from section 5.1 in mind.
		Sorting search result by field value
			By default, the search method that accepts a Sort argument won’t compute any scores for the matching documents. This is often a sizable performance gain, and many applications don’t need the scores when sorting by field. If scores aren’t needed in your application, it’s best to keep this default. If you need to change the default, use IndexSearcher’s setDefaultFieldSortScoring method, which takes two Booleans: doTrackScores and doMaxScore. If doTrackScores is true, then each hit will have a score computed. If doMaxScore is true, then the max score across all hits will be computed. Note that computing the max score is in general more costly than the score per hit, because the score per hit is only computed if the hit is competitive.For our example, because we want to display the scores, we enable score tracking but not max score tracking.
		Sorting by relevance
			Sorting by score works by either passing null as the Sort object or using the default sort behavior. Each of these variants returns results in the default score order. Sort.RELEVANCE is equivalent to new Sort(): example.displayResults(query, Sort.RELEVANCE); example.displayResults(query, new Sort());
			Notice how many of the hits have identical scores, but within blocks of identical scores the sort is by document ID ascending. Lucene internally always adds an implicit final sort, by document ID, to consistently break any ties in the sort order that you specified.
		Sorting by index order
			If the order in which the documents were indexed is relevant, you can use Sort.INDEXORDER
			Document order may be interesting for an index that you build up once and never change. But if you need to reindex documents, document order typically won’t work because newly indexed documents receive new document IDs and will be sorted last.
		Sorting by a field
			Sorting by a textual field first requires that the field was indexed as a single token, as described in section 2.4.6. Typically this means using Field.Index.NOT_ANALYZED or Field.Index.NOT_ANALYZED_NO_NORMS. Separately, you can choose whether or not to store the field.
			NumericField instances are automatically indexed properly for sorting. To sort by a field, you must create a new Sort object, providing the field name: example.displayResults(query, new Sort(new SortField("category", SortField.STRING)));
		Reversing sort order
			The default sort direction for sort fields (including relevance and document ID) is natural ordering. Natural order is descending for relevance but increasing for all other fields. The natural order can be reversed per Sort object by specifying true for the second argument.
		Sorting by multiple fields
			Sorting by multiple fields is important whenever your primary sort leaves ambiguity because there are equal values. Implicitly we’ve been sorting by multiple fields, because Lucene automatically breaks ties by document ID. You can control the sort fields explicitly by creating Sort with multiple SortFields.
			A SortField holds the field name, a field type, and the reverse order flag. SortField contains constants for several field types, including SCORE, DOC, STRING, BYTE, SHORT, INT, LONG, FLOAT, and DOUBLE. SCORE and DOC are special types for sorting on relevance and document ID.
		Selecting a sorting field type
			By search time, the fields that can be sorted on and their corresponding types are already set. Indexing time is when the decision about sorting capabilities should be made, but custom sorting implementations can do so at search time
		Using a nondefault local for sorting
			But if you need a different collation order, SortField lets you specify a Locale. A Collator instance is obtained for the provided locale using Collator.getInstance(Locale), and the Collator.compare method then determines the sort order.
			Both constructors imply the SortField.STRING type because the locale applies only to string-type sorting, not to numerics.
	Using MultiPhraseQuery
		The built-in MultiPhraseQuery is definitely a niche query, but it’s potentially useful.MultiPhraseQuery is just like PhraseQuery except that it allows multiple terms per position. You could achieve the same logical effect, albeit at a high performance cost, by enumerating all possible phrase combinations and using a BooleanQuery to “OR” them together.  For example, suppose we want to find all documents about speedy foxes, with quick or fast followed by fox. One approach is to do a "quick fox" OR "fast fox" query. Another option is to use MultiPhraseQuery.
		MultiPhraseQuery query = new MultiPhraseQuery(); query.add(new Term[] { new Term("field", "quick"), new Term("field", "fast") }); query.add(new Term("field", "fox"));	
		One difference between using MultiPhraseQuery and using PhraseQuery’s Boolean- Query is that the slop factor is applied globally with MultiPhraseQuery—it’s applied on a per-phrase basis with PhraseQuery.	
		Of course, hard-coding the terms wouldn’t be realistic, generally speaking. One possible use of a MultiPhraseQuery would be to inject synonyms dynamically into phrase positions, allowing for less precise matching. For example, you could tie in the WordNet-based code (see section 9.3 for more on WordNet and Lucene). As seen in listing 5.6, QueryParser produces a MultiPhraseQuery for search terms surrounded in double quotes when the analyzer it’s using returns positionIncrement 0 for any of the tokens within the phrase.	
	Querying on multiple fields at once
		The first approach is to create a multivalued catchall field to index the text from all fields, as we’ve done for the contents field in our book test index. Be sure to increase the position increment gap across field values, as described in section 4.7.1, to avoid incorrectly matching across two field values. You then perform all searching against the catchall field. This approach has some downsides: you can’t directly control per-field boosting1, and disk space is wasted, assuming you also index each field separately.
		The second approach is to use MultiFieldQueryParser, which subclasses Query- Parser. Under the covers, it instantiates a QueryParser, parses the query expression for each field, then combines the resulting queries using a BooleanQuery. The default operator OR is used in the simplest parse method when adding the clauses to the Boolean- Query. For finer control, the operator can be specified for each field as required (BooleanClause.Occur.MUST), prohibited (BooleanClause.Occur.MUST_NOT), or normal (BooleanClause.Occur.SHOULD), using the constants from BooleanClause.	
		MultiFieldQueryParser has some limitations due to the way it uses QueryParser. You can’t control any of the settings that QueryParser supports, and you’re stuck with the defaults, such as default locale date parsing and zero-slop default phrase queries.	
		If you choose to use MultiFieldQueryParser, be sure your queries are fabricated appropriately using the QueryParser and Analyzer diagnostic techniques shown in chapters 3 and 4. Plenty of odd interactions with analysis occur using QueryParser, and these are compounded when using MultiFieldQueryParser. An important downside of MultiFieldQueryParser is that it produces more complex queries, as Lucene must separately test each query term against every field, which will run slower than using a catchall field.	
		The third approach for automatically querying across multiple fields is the advanced DisjunctionMaxQuery, which wraps one or more arbitrary queries, OR’ing together the documents they match. You could do this with BooleanQuery, as Multi-FieldQueryParser does, but what makes DisjunctionMaxQuery interesting is how it scores each hit: when a document matches more than one query, it computes the score as the maximum score across all the queries that matched, compared to BooleanQuery, which sums the scores of all matching queries. This can produce better end-user relevance.	
		DisjunctionMaxQuery also includes an optional tie-breaker multiplier so that, all things being equal, a document matching more queries will receive a higher score than a document matching fewer queries. To use DisjunctionMaxQuery to query across multiple fields, you create a new field-specific Query, for each field you’d like to include, and then use DisjunctionMaxQuery’s add method to include that Query.	
		Which approach makes sense for your application? The answer is “It depends,” because there are important trade-offs. The catchall field is a simple index time–only solution but results in simplistic scoring and may waste disk space by indexing the same text twice. Yet it likely yields the best searching performance. MultiFieldQuery- Parser produces BooleanQuerys that sum the scores (whereas DisjunctionMaxQuery takes the maximum score) for all queries that match each document, then properly implements per-field boosting.	
	Span Queries
		A span in this context is a starting and ending token position in a field. Recall from section 4.2.1 that tokens emitted during the analysis process include a position from the previous token. This position information, in conjunction with the new SpanQuery subclasses, allows for even more query discrimination and sophistication, such as all documents where “President Obama” is near “health care reform.”
		Using the query types we’ve discussed thus far, it isn’t possible to formulate such a position-aware query. You could get close with something like "president obama" AND "health care reform", but these phrases may be too distant from one another withinthe document to be relevant for our searching purposes. In typical applications, Span- Querys are used to provide richer, more expressive position-aware functionality than PhraseQuery.	
		While searching, span queries track more than the documents that match: the individual spans, perhaps more than one per field, are also tracked. Contrasting with TermQuery, which simply matches documents, SpanTermQuery matches exactly the same documents but also keeps track of the positions of every term occurrence that matches. Generally this is more compute-intensive. For example, when TermQuery finds a document containing its term, it records that document as a match and immediately moves on, whereas SpanTermQuery must enumerate all the occurrences of that term within the document.	
		SpanQuery family:
			SpanQuery type 				Description
			SpanTermQuery 				Used in conjunction with the other span query types. On its own, it’s functionally equivalent to TermQuery.
			SpanFirstQuery 				Matches spans that occur within the first part of a field.
			SpanNearQuery 				Matches spans that occur near one another.
			SpanNotQuery 				Matches spans that don’t overlap one another.
			FieldMaskingSpanQuery 		Wraps another SpanQuery but pretends a different field was matched. This is useful for doing span matches across fields, which is otherwise not possible.
			SpanOrQuery 				Aggregates matches of span queries.
		Building block of spanning, SpanTermQuery
			Span queries need an initial leverage point, and SpanTermQuery is just that. Internally, a SpanQuery keeps track of its matches: a series of start/end positions for each matching document. By itself, a SpanTermQuery matches documents just like TermQuery does, but it also keeps track of position of the same terms that appear within each document. Generally you’d never use this query by itself (you’d use TermQuery instead); you only use it as inputs to the other SpanQuery classes.
			Spans spans = query.getSpans(reader); spans.next(); spans.start(); spans.end();
		Finding spans at the begining of a field
			To query for spans that occur within the first specific number of positions of a field, use SpanFirstQuery.
			Any SpanQuery can be used within a SpanFirstQuery, with matches for spans that have an ending position in the first specified number (2 and 3 in this case) of positions.
		Spans near one another
			A PhraseQuery (see section 3.4.6) matches documents that have terms near one another, with a slop factor to allow for intermediate or reversed terms. SpanNearQuery operates similarly to PhraseQuery, with some important differences. SpanNearQuery matches spans that are within a certain number of positions from one another, with a separate flag indicating whether the spans must be in the order specified or can be reversed. The resulting matching spans span from the start position of the first span sequentially to the ending position of the last span.
			Using SpanTermQuery objects as the SpanQuerys in a SpanNearQuery is much like using a PhraseQuery. The SpanNearQuery slop factor is a bit less confusing than the PhraseQuery slop factor because it doesn’t require at least two additional positions toaccount for a reversed span. To reverse a SpanNearQuery, set the inOrder flag (third argument to the constructor) to false.
			We’ve only shown SpanNearQuery with nested SpanTermQuerys, but SpanNearQuery allows for any SpanQuery type.
		Excluding span overlap from matches
			The SpanNotQuery excludes matches where one SpanQuery overlaps another.
			The first argument to the SpanNotQuery constructor is a span to include, and the second argument is a span to exclude.
		SpanOrQuery
			SpanOrQuery, which aggregates an array of SpanQuerys. Our example query, in English, is all documents that have “quick fox” near “lazy dog” or that have “quick fox” near “sleepy cat.” The first clause of this query is shown in figure 5.4. This single clause is SpanNearQuery nesting two SpanNearQuerys, and each consists of two SpanTermQuerys.
			SpanNearQuery quick_fox = new SpanNearQuery(new SpanQuery[]{quick, fox}, 1, true);
			SpanNearQuery lazy_dog = new SpanNearQuery(new SpanQuery[]{lazy, dog}, 0, true);
			SpanNearQuery sleepy_cat = new SpanNearQuery(new SpanQuery[]{sleepy, cat}, 0, true);
			SpanNearQuery qf_near_ld = new SpanNearQuery(new SpanQuery[]{quick_fox, lazy_dog}, 3, true);
			assertOnlyBrownFox(qf_near_ld);
			dumpSpans(qf_near_ld);
			SpanNearQuery qf_near_sc =
			new SpanNearQuery(new SpanQuery[]{quick_fox, sleepy_cat}, 3, true);
			dumpSpans(qf_near_sc);
			SpanOrQuery or = new SpanOrQuery(new SpanQuery[]{qf_near_ld, qf_near_sc});
			assertBothFoxes(or);
			Both SpanNearQuery and SpanOrQuery accept any other SpanQuerys, so you can create arbitrarily nested queries. For example, imagine you’d like to perform a “phrase within a phrase” query, such as a subphrase query "Bob Dylan", with the slop factor 0 for an exact match, and an outer phrase query matching this phrase with the word sings, with a nonzero slop factor. Such a query isn’t possible with PhraseQuery, because it only accepts terms. But you can easily create this query by embedding one SpanNearQuery within another.
		SpanQuery and QueryParser
			QueryParser doesn’t currently support any of the SpanQuery types, but the surround QueryParser in Lucene’s contrib modules does.
	Filtering a search
		Filtering is a mechanism of narrowing the search space, allowing only a subset of the documents to be considered as possible hits.
		Consider also the alternative to using a filter: aggregating required clauses in a BooleanQuery.	
		Before you get concerned about mentions of caching results, rest assured that it’s done with a tiny data structure (a DocIdBitSet) where each bit position represents a document.	
		There are numerous built-in filter implementations:
			?? TermRangeFilter matches only documents containing terms within a specified range of terms. It’s exactly the same as TermRangeQuery, without scoring.
			?? NumericRangeFilter matches only documents containing numeric values within a specified range for a specified field. It’s exactly the same as Numeric- RangeQuery, without scoring.
			?? FieldCacheRangeFilter matches documents in a certain term or numeric range, using the FieldCache (see section 5.1) for better performance.
			?? FieldCacheTermsFilter matches documents containing specific terms, using the field cache for better performance. 
			?? QueryWrapperFilter turns any Query instance into a Filter instance, by using only the matching documents from the Query as the filtered space, discarding the document scores.
			?? SpanQueryFilter turns a SpanQuery into a SpanFilter, which subclasses the base Filter class and adds an additional method, providing access to the positional spans for each matching document. This is just like QueryWrapperFilter but is applied to SpanQuery classes instead.
			?? PrefixFilter matches only documents containing terms in a specific field with a specific prefix. It’s exactly the same as PrefixQuery, without scoring.
			?? CachingWrapperFilter is a decorator over another filter, caching its results to increase performance when used again.
			?? CachingSpanFilter does the same thing as CachingWrapperFilter, but it caches a SpanFilter.
			?? FilteredDocIdSet allows you to filter a filter, one document at a time. In order to use it, you must first subclass it and define the match method in your subclass.
		TermRangeFilter
			TermRangeFilter filters on a range of terms in a specific field, just like TermRange- Query minus the scoring. If the field is numeric, you should use NumericRangeFilter (described next) instead. TermRangeFilter applies to textual fields.
			The first parameter to both of the TermRangeFilter constructors is the name of the field in the index. The two final Boolean arguments to the constructor for TermRangeFilter, includeLower, and includeUpper determine whether the lower and upper terms should be included or excluded from the filter.
			TermRangeFilter also supports open-ended ranges. To filter on ranges with one end of the range specified and the other end open, just pass null for whichever end should be open
				filter = new TermRangeFilter("modified", null, jan31, false, true);
				filter = new TermRangeFilter("modified", jan1, null, true, false);
			TermRangeFilter provides two static convenience methods to achieve the same thing:
				filter = TermRangeFilter.Less("modified", jan31);
				filter = TermRangeFilter.More("modified", jan1);
		NumericRangeFilter
			NumericRangeFilter filters by numeric value. This is just like NumericRangeQuery, minus the constant scoring
			The same caveats as NumericRangeQuery apply here; for example, if you specify a precisionStep different from the default, it must match the precisionStep used during indexing.
		FieldCacheRangeFilter
			FieldCacheRangeFilter is another option for range filtering. It achieves exactly the same filtering as both TermRangeFilter and NumericRangeFilter, but does so by using Lucene’s field cache. This may result in faster performance in certain situations, since all values are preloaded into memory. But the usual caveats with field cache apply
			FieldCacheRangeFilter exposes a different API to achieve range filtering. 
				Filter filter = FieldCacheRangeFilter.newStringRange("title2", "d", "j", true, true);
				filter = FieldCacheRangeFilter.newIntRange("pubmonth", 201001, 201006, true, true)
		Filtering by specific terms
			Sometimes you’d simply like to select specific terms to include in your filter.
			The first approach is FieldCacheTermsFilter, which uses field cache under the hood. Simply instantiate it with the field (String) and an array of String
				Filter filter = new FieldCacheTermsFilter("category", new String[] {"/health/alternative/chinese", "/technology/computers/ai", "/technology/computers/programming"});
			All documents that have any of the terms in the specified field will be accepted. Note that the documents must have a single term value for each field. Under the hood, this filter loads all terms for all documents into the field cache the first time it’s used during searching for a given field. This means the first search will be slower, but subsequent searches, which reuse the cache, will be very fast. The field cache is reused even if you change which specific terms are included in the filter.
			The second approach for filtering by terms is TermsFilter, which is included in Lucene’s contrib modules. Terms- Filter doesn’t do any internal caching, and it allows filtering on fields that have more than one term; otherwise, TermsFilter and FieldCacheTermsFilter are functionally identical.
		Using QueryWrapperFilter
			QueryWrapperFilter uses the matching documents of a query to constrain available documents from a subsequent search. It allows you to turn a query, which does scoring, into a filter, which doesn’t.
				TermQuery categoryQuery = new TermQuery(new Term("category", "/philosophy/eastern"));
				Filter categoryFilter = new QueryWrapperFilter(categoryQuery);
		Using SpanQueryFilter
			SpanQueryFilter does the same thing as QueryWrapperFilter, except that it’s able to preserve the spans for each matched document.
				SpanQuery categoryQuery = new SpanTermQuery(new Term("category", "/philosophy/eastern")); 
				Filter categoryFilter = new SpanQueryFilter(categoryQuery);
			SpanQueryFilter adds a method, bitSpans, enabling you to retrieve the spans for each matched document. Only advanced applications will make use of spans (Lucene doesn’t use them internally when filtering), so if you don’t need this information, it’s better (faster) to simply use QueryWrapperFilter.
		Security filters
			If your security requirements are this straightforward, where documents can be associated with users or roles during indexing, using a QueryWrapperFilter will work nicely. But some applications require more dynamic enforcement of entitlements.
		Using BooleanQuery for filtering
			You can constrain a query to a subset of documents another way, by combining the constraining query to the original query as a required clause of a BooleanQuery. There are a couple of important differences, despite the fact that the same documents are returned from both. If you use CachingWrapperFilter around your QueryWrapper- Filter, you can cache the set of documents allowed, likely speeding up successive searches using the same filter. In addition, normalized document scores are unlikely to be the same. The score difference makes sense when you’re looking at the scoring formula (see section 3.3) because the IDF (inverse document frequency) factor may be dramatically different. When you’re using BooleanQuery aggregation, all documents containing the terms are factored into the equation, whereas a filter reduces the documentsunder consideration and impacts the inverse document frequency factor.
		PrefixFilter
			PrefixFilter, the corollary to PrefixQuery, matches documents containing Terms starting with a specified prefix. Filter prefixFilter = new PrefixFilter(new Term("category", "/technology/computers"));
		Caching filter results
			The biggest benefit from filters comes when they’re cached and reused using CachingWrapperFilter, which takes care of caching automatically (internally using a WeakHashMap, so that externally dereferenced entries get garbage collected). You can cache any filter using CachingWrappingFilter. Filters cache by using the IndexReader as the key, which means searching should also be done with the same instance of IndexReader to benefit from the cache. If you aren’t constructing IndexReader yourself but are creating an IndexSearcher from a directory, you must use the same instance of IndexSearcher to benefit from the caching. When index changes need to be reflected in searches, discard IndexSearcher and IndexReader and reinstantiate.
			Filter filter = new TermRangeFilter("title2", "d", "j", true, true);
			CachingWrapperFilter cachingFilter;
			cachingFilter = new CachingWrapperFilter(filter);
		Wrapping a filter as a query
			We saw how to wrap a filter as a query. You can also do the reverse, using Constant- ScoreQuery to turn any filter into a query, which you can then search on. The resulting query matches only documents that are included in the filter, and assigns all of them the score equal to the query boost.
		Filtering a filter
			The FilteredDocIdSet class is an abstract class that accepts a primary filter, and then, during matching whenever a document is being considered, the match method (of your subclass) is invoked to check whether the document should be allowed. This allows you to dynamically filter any other filter by implementing any custom logic in your match method. This approach is efficient because FilteredDocIdSet never fully materializes a bit set for the filter. Instead, each match is checked on demand.
			This approach can be useful for enforcing entitlements, especially in cases where much of the enforcement is static (present in the index) but some amount of entitlement should be checked dynamically at runtime. For such a use case, you’d create a standard entitlements filter based on what’s in the index, then subclass FilteredDoc- IdSet, overriding the match method, to implement your dynamic entitlements logic.
		Beyond the build-in filters
			Lucene isn’t restricted to using the built-in filters. An additional filter found in the Lucene contrib modules, ChainedFilter, allows for complex chaining of filters.
			And if these filtering options aren’t enough, Lucene adds another interesting use of a filter. The FilteredQuery filters a query, like IndexSearcher’s search(Query, Filter, int) can, except it is itself a query: therefore it can be used as a single clause within a BooleanQuery. Using FilteredQuery seems to make sense only when using custom filters
	Custom scoring using function queries	
		Function query classes
			The base class for all function queries is ValueSourceQuery. This is a query that matches all documents but sets the score of each document according to a Value- Source provided during construction. The function package provides Field- CacheSource, and its subclasses, to derive values from the field cache. You can also create your own ValueSource—for example, to derive scores from an external database. But probably the simplest approach is to use FieldScoreQuery, which subclasses ValueSourceQuery and derives each document’s score statically from a specific indexed field. The field should be a number, indexed without norms and with a single token per document. Typically you’d use Field.Index.NOT_ANALYZED_NO_NORMS.
			Note that the IndexReader argument provided to the getCustomScoreProvider method is per-segment, meaning the method will be called multiple times during searching if the index has more than one segment. This is important as it enables your scoring logic to efficiently use the per-segment reader to retrieve values in the fieldcache.
			CustomScoreQuery. This query class lets you combine a normal Lucene query with one or more other function queries.
		Boosting recently modified documents using function queries
	Searching across multiple Lucene indexes
		Lucene provides two useful classes for searching across multiple indexes. We’ll first meet MultiSearcher, which uses a single thread to perform searching across multiple indexes. Then we’ll see ParallelMultiSearcher, which uses multiple threads to gain concurrency.
		Using MultiSearcher
			With MultiSearcher, all indexes can be searched with the results merged in a specified (or descending-score, by default) order. Using MultiSearcher is comparable to using IndexSearcher, except that you hand it an array of IndexSearchers to search rather than a single directory (so it’s effectively a decorator pattern and delegates most of the work to the subsearchers).
				searchers = new IndexSearcher[2];
				searchers[0] = new IndexSearcher(aTOmDirectory);
				searchers[1] = new IndexSearcher(nTOzDirectory);
				MultiSearcher searcher = new MultiSearcher(searchers);
		Multithreaded searching using ParallelMultiSearcher
			A multithreaded version of MultiSearcher, called ParallelMultiSearcher, spawns a new thread for each Searchable and waits for them all to finish when the search method is invoked. The basic search and search with filter options are parallelized, but searching with a Collector hasn’t yet been parallelized. The exposed API is the same as MultiSearcher, so it’s a simple drop-in.
			Whether you’ll see performance gains using ParallelMultiSearcher depends on your architecture. If the indexes reside on different physical disks and your computer has CPU concurrency, you should see improved performance. But there hasn’t been much real-world testing to back this up, so be sure to test it for your application.
			A cousin to ParallelMultiSearcher lives in Lucene’s contrib/remote directory, enabling you to remotely search multiple indexes in parallel.
	Leveraging term vectors
		what you can do at search time once you have term vectors in the index: finding similar documents and automatically categorizing documents.
		Technically, a term vector is a collection of termfrequency pairs, optionally including positional information for each term occurrence. Most of us probably can’t picture vectors in hyper-dimensional space, so for visualization purposes, let’s look at two documents that contain only the terms cat and dog. These words appear various times in each document. Plotting the term frequencies of each document in X, Y coordinates looks something like figure 5.5. What gets interesting with term vectors is the angle between them
		A TermFreqVector instance has several methods for retrieving the vector information, primarily as matching arrays of Strings and ints (the term value and frequency in the field, respectively). If you had also stored offsets and/or positions information with your term vectors, using Field.TermVector.WITH_POSITIONS_OFFSETS for example, then you’ll get a TermPositionVector back when you load the term vectors. That class contains offset and position information for each occurrence of the terms in the document.
		Books like this
			Lucene’s contrib modules contains a useful Query implementation, More- LikeThisQuery, doing the same thing as our BooksLikeThis class but more generically. BooksLikeThis is clearly hardwired to fields like subject and author from our books index. But MoreLikeThisQuery lets you set the field names, so it works well on any index.
		What category?
		TermVectorMapper
			The methods that a concrete TermVectorMapper implementation (subclass) must implement.
				Method 					Purpose
				setDocumentNumber 		Called once per document to tell you which document is currently being loaded.
				setExpectations 		Called once per field to tell you how many terms occur in the field, and whether positions and offsets are stored.
				map 					Called once per term to provide the actual term vectors data.
				isIgnoringPositions 	You should return false only if you need to see the positions data for the term vectors.
				isIgnoringOffsets 		You should return false only if you need to see the offsets data for the term vectors.
			Built-in implementations of TermVectorMapper
				Method 								Purpose
				PositionBasedTermVectorMapper 		For each field, stores a map from the integer position to terms and optionally offsets that occurred at that position.
				SortedTermVectorMapper 				Merges term vectors for all fields into a single SortedSet, sorted according to a Comparator that you specify. One comparator is provided in the Lucene core, TermVector-EntryFreqSortedComparator, which sorts first by frequency of the term and second by the term itself.
				FieldSortedTermVectorMapper 		Just like SortedTermVectorMapper, except the fields aren’t merged together and instead each field stores its sorted terms separately.
	Loading fields with FieldSelector
		When presenting the search results, you might only need the metadata fields and so loading the very large fields is costly and unnecessary. This is where FieldSelector comes in. FieldSelector, which is in the org.apache. lucene.document package, allows you to load a specific restricted set of fields for each document. It’s an interface with a single simple method: FieldSelectorResult accept(String fieldName); Concrete classes implementing this interface return a FieldSelectorResult describing whether the specified field name should be loaded, and how.
		FieldSelector-Result is an enum with seven values:
			Option 				Purpose
			LOAD 				Load the field.
			LAZY_LOAD 			Load the field lazily. The actual contents of the field won’t be read until Field.stringValue() or Field.binaryValue() is called.
			NO_LOAD 			Skip loading the field.
			LOAD_AND_BREAK 		Load this field and don’t load any of the remaining fields.
			LOAD_FOR_MERGE 		Used internally to load a field during segment merging; this skips decompressing compressed fields.
			SIZE 				Read only the size of the field, then add a binary field with a 4-byte array encoding that size.
			SIZE_AND_BREAK 		Like SIZE, but don’t load any of the remaining fields.
		When loading stored fields with a FieldSelector, IndexReader steps through the fields one by one for the document, in the order they had originally been added to the document during indexing, invoking FieldSelector on each field and choosing to load the field (or not) based on the returned result.
		Core FieldSelector implementations:
			Class 							Purpose
			LoadFirstFieldSelector 			Loads only the first field encountered.
			MapFieldSelector 				You specify the String names of the fields you want to load; all other fields are skipped.
			SetBasedFieldSelector 			You specify two sets: the first set is fields to load and the second set is fields to load lazily.
		Although FieldSelector will save time when loading fields, just how much time is application-dependent. Much of the cost when loading stored fields is in seeking the file pointers to the places in the index where all fields are stored, so you may find you don’t save that much time skipping fields. Test on your application to find the right trade-off.
	Stopping a slow search
		Lucene has a special Collector implementation, TimeLimiting- Collector, that stops a search when it has taken too much time.
		TimeLimitingCollector delegates all methods to a separate Collector that you provide, and throws a TimeExceededException when the searching has taken too long.
			TopScoreDocCollector topDocs = TopScoreDocCollector.create(10, false);
			Collector collector = new TimeLimitingCollector(topDocs, 1000);
		There are a few couple of limitations to TimeLimitingCollector. First, it adds some of its own overhead during results collection (to check the timeout, per matched document) and that will make your searches run somewhat slower, though the impact should be small. Second, it only times out the search during collection, whereas it’s possible that some queries take a very long time during Query.rewrite(). For such queries it’s possible you won’t hit the TimeExceededException until well after your requested timeout.
	Summary
		

Extending Search
	Using a custom sort method
		Lucene lets you implement a custom sorting mechanism by providing your own subclass of the FieldComparatorSource abstract base class. Custom sorting implementations are most useful in situations when the sort criteria can’t be determined during indexing.
		Indexing documents for geographic sorting
			Note that Lucene now includes the “spatial” package in the contrib modules, described in section 9.7, for filtering and sorting according to geographic distance in general.
		Implementing custom geographic sort
			The sorting infrastructure within Lucene interacts with the FieldComparatorSource and FieldComparator B, E API in order to sort matching documents. For performance reasons, this API is more complex than you’d otherwise expect. In particular, the comparator is made aware of the size of the queue (passed as the numHits argument to newComparator) D being tracked within Lucene. In addition, the comparator is notified every time a new segment is searched (with the setNextReader method).
		Accessing values used in custom sorting
			The IndexSearcher.search method you use when sorting, covered in section 5.2, returns more information than the top documents: public TopFieldDocs search(Query query, Filter filter, int nDocs, Sort sort) TopFieldDocs is a subclass of TopDocs that adds the values used for sorting each hit. The values are available via each FieldDoc, which subclasses ScoreDoc, contained in the array of returned results. FieldDoc encapsulates the computed raw score, document ID, and an array of Comparables with the value used for each SortField.
			The total number of hits is still provided because all hits need to be determined to find the three best ones.
	Developing a custom Collector
		Lucene allows full customization of what you do with each matching document if you create your own subclass of the abstract Collector base class. For example, perhaps you wish to gather every single document ID that matched the query.
		Methods to implement for a custom Collector:
			Method name 											Purpose
			setNextReader(IndexReader reader, int docBase) 			Notifies the collector that a new segment is being searched, and provides the segment’s IndexReader and the starting base for documents.
			setScorer(Scorer scorer) 								Provides a Scorer to the Collector. This is also called once per segment. The Collector should call Scorer.score() from within its collect() method to retrieve the score for the current matched document.
			collect(int docID) 										Called for each document that matches the search. The docID is relative to the current segment, so docBase must be added to it to make it absolute.
			acceptsDocsOutOfOrder() 								Return true if your Collector can handle out-of-order docIDs. Some BooleanQuery instances can collect results faster if this returns true.
		The Collector base class
			Collector is an abstract base class that defines the API that Lucene interacts with while doing searching. As with the FieldComparator API for custom sorting, Collector’s API is more complex than you’d expect, in order to enable high-performance hit collection.
			All of Lucene’s core search methods use a Collector subclass under the hood to do their collection. For example, when sorting by relevance, TopScoreDocCollector is used. When sorting by field, it’s TopFieldCollector. Both of these are public classes in the org.apache.lucene.search package, and you can instantiate them yourself if needed.
			During searching, when Lucene finds a matching document, it calls the Collector’s collect(int docID) method. Lucene couldn’t care less what’s done with the document; it’s up to the Collector to record the match, if it wants. This is the hot spot of searching, so make sure your collect method does only the bare minimum work required.
			Lucene drives searching one segment at a time, for higher performance, and notifies you of each segment transition by calling the setNextReader(IndexReaderreader, int docBase). The provided IndexReader is specific to the segment. It will be a different instance for each segment. It’s important for the Collector to record the docBase at this point, because the docID provided to the collect method is relative within each segment. To get the absolute or global docID, you must add docBase to it. This method is also the place to do any segment-specific initialization required by your collector.
			Note that the relevance score isn’t passed to the collect method. This saves wasted CPU for Collectors that don’t require it. Instead, Lucene calls the setScorer(Scorer) method on the Collector, once per segment in the index, to provide a Scorer instance. You should hold onto this Scorer, if needed, and then retrieve the relevance score of the currently matched document by calling Scorer.score(). That method must be called from within the collect method because it holds volatile data specific to the current docID being collected. Note that Scorer.score() will recompute the score every time, so if your collect method may invoke score multiple times, you should call it once internally and simply reuse the returned result. Alternatively, Lucene provides the ScoreCachingWrapperScorer, which is a Scorer implementation that caches the score per document. Note also that Scorer is a rich and advanced API in and of itself, but in this context you should only use the score method.		
			The final method, acceptsDocsOutOfOrder(), which returns a Boolean, is invoked by Lucene to see whether your Collector can tolerate docIDs that arrive out of sorted order. Many collectors can, but some collectors either can’t accept docIDs out or order, or would have to do too much extra work. If possible, you should return true, because certain BooleanQuery instances can use a faster scorer under the hood if given this freedom.
		Custom collector: BookLinkCollector
		AllDocCollector
	Extending QueryParser
		QueryParser is also extensible, allowing subclassing to override parts of the query-creation process.
		Customizing QueryParser’s behavior
			QueryParser’s extensibility points:
				Method 																																					Why override?
				getFieldQuery(String field, Analyzer analyzer, String queryText) or getFieldQuery(String field, Analyzer analyzer, String queryText, int slop) 			These methods are responsible for the construction of either a TermQuery or a PhraseQuery. If special analysis is needed, or a unique type of query is desired, override this method. For example, a SpanNearQuery can replace PhraseQuery to force ordered phrase matches.
				getFuzzyQuery(String field, String termStr, float minSimilarity) 																						Fuzzy queries can adversely affect performance. Override and throw a ParseException to disallow fuzzy queries.
				getPrefixQuery(String field, String termStr) 																											This method is used to construct a query when the term ends with an asterisk. The term string handed to this method doesn’t include the trailing asterisk and isn’t analyzed. Override this method to perform any desired analysis.
				getRangeQuery(String field, String start, String end, boolean inclusive) 																				Default range-query behavior has several noted quirks (see section 3.5.3). Overriding could lowercase the start and end terms, use a different date format, or handle number ranges by converting to a NumericRangeQuery (see section 6.3.3).
				getBooleanQuery(List clauses) or getBooleanQuery(List clauses, boolean disableCoord) 																	Constructs a BooleanQuery given the clauses.
				getWildcardQuery(String field, String termStr) 																											Wildcard queries can adversely affect performance, so overridden methods could throw a ParseException to disallow them. Alternatively, because the term string isn’t analyzed, special handling may be desired.
			All of the methods listed return a Query, making it possible to construct something other than the current subclass type used by the original implementations of these methods. Also, each of these methods may throw a ParseException, allowing for error handling.
			QueryParser also has extensibility points for instantiating each query type. These differ from the points listed in table 6.2 in that they create the requested query type and return it. Overriding them is useful if you only want to change which Query class is used for each type of query without altering the logic of what query is constructed. These methods are newBooleanQuery, newTermQuery, newPhraseQuery, newMultiPhraseQuery, newPrefixQuery, newFuzzyQuery, newRangeQuery, newMatch- AllDocsQuery and newWildcardQuery. For example, if whenever a TermQuery is created by QueryParser you’d like to instantiate your own subclass of TermQuery, simply override newTermQuery.
		Prohibiting fuzzy and wildcard queries
		Handling numeric field-range queries
			Unfortunately, QueryParser is unable to produce the corresponding NumericRangeQuery instances at search time. Fortunately, it’s simple to subclass QueryParser to do so
		Handling date ranges
			Typically the client’s locale would be determined and used instead of the default locale. For example, in a web application the HttpServletRequest object contains the locale set by the client browser.
		Allowing ordered phrase queries
	Custom filters
		If all the information needed to perform filtering is in the index, there’s no need to write your own filter because the QueryWrapperFilter can handle it
		Implementing a custom filter
		Using our custom filter during searching
		An alternative: FilteredQuery
			To add to the filter terminology overload, one final option is FilteredQuery.3 FilteredQuery inverts the situation that searching with a filter presents. Using a filter, an IndexSearcher’s search method applies a single filter during querying. Using the FilteredQuery, though, you can turn any filter into a query, which opens up neat possibilities, such as adding a filter as a clause to a BooleanQuery.
	Payloads
		Payloads, an advanced feature in Lucene, enable an application to store an arbitrary byte array for every occurrence of a term during indexing. This byte array is entirely opaque to Lucene: it’s simply stored at each term position, during indexing, and then can be retrieved during searching. Otherwise the core Lucene functionality doesn’t do anything with the payload or make any assumptions about its contents. This means you can store arbitrary encoded data that’s important to your application, and then use it during searching, either to decide which documents are included in the search results or to alter how matched documents are scored or sorted.
		All sorts of uses cases are enabled with payloads. One example, which we delve into in this section, is boosting the same term differently depending on where it occurred in the document. Another example is storing part-of-speech information for each term in the index, and altering how filtering, scoring, or sorting is done based on that. By creating a single-term field, you can store document-level metadata, such as an application-specific unique identifier. Yet another example is storing formatting information that was lost during analysis, such as whether a term was bold or italic, or what font or font size was used.
		Position-specific boosting allows you to alter the score of matched documents when the specific occurrences of each term were “important.” Imagine we’re indexing mixed documents, where some of them are bulletins (weather warnings) and others are more ordinary documents. You’d like a search for “warning” to give extra boost when it occurs in a bulletin document. Another example is boosting terms that were bolded or italicized in the original text, or that were contained within a title or header tag for HTML documents. Although you could use field boosting to achieve this, that’d require you to separate all the important terms into entirely separate fields, which is often not feasible or desired. The payloads feature lets you solve this by boosting on a term-by-term basis within a single field.
		Producing payloads during analysis
			The first step is to create an analyzer that detects which tokens are important and attaches the appropriate payloads. The TokenStream for such an analyzer should define the PayloadAttribute, and then create a Payload instance when appropriate and set the payload using PayloadAttribute.setPayload inside the incrementToken method.
			TokenFilter in contrib/analyzers that encode certain TokenAttributes as payloads:
			Name 							Purpose
			NumericPayloadTokenFilter 		Encodes a float payload for those tokens matching the specified token type
			TypeAsPayloadTokenFilter 		Encodes the token’s type as a payload on every token
			TokenOffsetPayloadTokenFilter 	Encodes the start and end offset of each token into its payload
			PayloadHelper 					Static methods to encode and decode ints and floats into byte array payloads
		Using payloads during searching
			Note that the payloads package also includes PayloadNearQuery, which is just like SpanNearQuery except it invokes Similarity.scorePayload just like PayloadTerm- Query.
		Payloads and SpanQuery
			Although using PayloadTermQuery and PayloadNearQuery is the simplest way to use payloads to alter scoring of documents, all of the SpanQuery classes allow expert access to the payloads that occur within each matching span returned by the getSpans method. At this point, none of the SpanQuery classes, besides SpanTermQuery and SpanNearQuery, have subclasses that make use of the payloads. It’s up to you to subclass a SpanQuery class and override the getSpans method if you’d like to filter documents that match based on payload, or override the SpanScorer class to provide custom scoring based on the payloads contained within each matched span.
		Retrieving payloads via TermPositions
			The final Lucene API that has been extended with payloads is the TermPositions iterator. This is an advanced internal API that allows you to step through the documents containing a specific term, retrieving each document that matched along with all positions, as well as their payload, of that term’s occurrences in the document.
	Summary

	
Apply Lucene
Extracting text with Tika
Essential Lucene extensions
	Luke, the Lucene Index Toolbox
		Luke has become a regular part of our Lucene development toolkit. Its tabbed and well-integrated UI allows for rapid browsing and experimentation.
		Overview: seeing the big picture
		Document browsing
		Using QueryParser to search
		Files and plugins view
	Analyzers, tokenizers, and TokenFilters
		The more analyzers, the merrier, we always say. And the contrib modules don’t disappoint in this area: they house numerous language-specific analyzers, a few related filters and tokenizers, and the slick Snowball algorithm analyzers.
		SnowballAnalyzer
		Ngram filters
			The ngram filters take a single token and emit a series of letter ngram tokens, which are combinations of adjacent letters as separate tokens.
		Shingle filters
			Shingles are single tokens constructed from multiple adjacent tokens. They’re similar to letter ngrams, used by the spellchecker package (section 8.5) and the ngram tokenizers (section 8.2.2) in that they make new tokens by combining multiple adjacent things. But whereas the ngram tokenizers operate on letters, shingles operate on whole words. For example, the sentence “please divide this sentence into shingles” might be tokenized into the shingles “please divide,” “divide this,” “this sentence,” “sentence into,” and “into shingles.”
		Obtaining the contrib analyzers
	Highlighting query terms
		Highlighter components
			TOKENSOURCES
				To create the TokenStream, you could reanalyze the text, using the same analyzer you had used during indexing.
			FRAGMENTER
				Fragmenter is a Java interface in the Highlighter package whose purpose is to split the original string into separate fragments for consideration.
			SCORER
				The output of the Fragmenter is a series of text fragments from which Highlighter must pick the best one(s) to present. To do this, Highlighter asks the Scorer, a Java interface, to score each fragment. The Highlighter package provides two concrete implementations: QueryTermScorer, which scores each fragment based on how many terms from the provided Query appear in the fragment, and QueryScorer, which attempts to only assign scores to actual term occurrences that contributed to the match for the document. When combined with SimpleSpanFragmenter, QueryScorer is usually the best option because true matches are highlighted.
			ENCODER
				The Encoder Java interface has a simple purpose: to encode the original text into the external format. There are two concrete implementations: DefaultEncoder, which is used by default in Highlighter, does nothing with the text. SimpleHTMLEncoder encodes the text as HTML, escaping any special characters such as < and > and &, and non-ASCII characters. Once the encoder is done, the final step is to format the fragments for presentation.
			FORMATTER
				The Formatter Java interface takes each fragment of text as a String, as well as the terms to be highlighted, and renders the highlighting. Highlighter provides three concrete classes to choose from. SimpleHTMLFormatter wraps a begin and end tag around each hit. The default constructor will use the <b> (bold) HTML tag. Gradient- Formatter uses different shades of background color to indicate how strong each hit was, using the <font> HTML tag. SpanGradientFormatter does the same thing but uses the <span> HTML tag because some browsers may not render the <font> tag correctly.
		Standalone highlighter example
		Highlighting with CSS
		Highlighting search results
	FastVectorHighlighter
		The advantages of FastVectorHighlighter over Highlighter are not only speed but also functionality. First, FastVectorHighlighter can support fields that are tokenized by ngram tokenizers. Highlighter can’t support such fields very well. Second, more interesting is that FastVectorHighlighter can output the multicolored tag highlighting out of the box, as shown in figure 8.9. Third, FastVectorHighlighter can support “per phrase” tagging, rather than the “per term” tagging that Highlighter supports. For instance, if you search the phrase “lazy dog,” FastVectorHighlighter produces <b>lazy dog</b> whereas Highlighter produces <b>lazy</b> <b>dog</b>.
	Spell checking
		Generating a suggestions list
		Selecting the best suggestion
		Presenting the result to the user
		Some ideas to improve spell checking
	Fun and interesting Query extensions
		MoreLikeThis
		FuzzyLikeThisQuery
		BoostingQuery
		TermsFilter
			TermsFilter is a filter that matches any arbitrary set of terms you specify. It’s like a TermRangeFilter that doesn’t require the terms to be in a contiguous sequence. You simply construct the TermsFilter, add the terms one by one you’d like to filter on by calling the addTerm method, and then use that filter when searching. An example might be a collection of primary keys from a database query result or perhaps a choice of “category” labels picked by the end user.
		DuplicateFilter
			DuplicateFilter is a Filter that removes documents that have the same value for a specific unanalyzed field. For example, say you have a field KEY, which isn’t analyzed but is indexed.
		RegexQuery
			RegexQuery, which is in the contrib/regex directory, allows you to specify an arbitrary regular expression for matching terms. Any document containing a term matching that regular expression will match. It’s like WildcardQuery on steroids.
			There are two books that match the cryptic regular expression .*st.*. By default RegexQuery uses Java’s built-in regular expression syntax, from java.util.regex, but you can switch to Apache Jakarta’s regular expression syntax (org.apache.regexp) by calling RegexQuery.setRegexImplementation(new JakartaRegexpCapabilities());
			The contrib/regex package also contains SpanRegexQuery, which combines Regex- Query and SpanQuery so that all matches also include the matching spans.
	Building contrib modules
		Get the sources
		Ant in the contrib directory
	Summary

	
Further Lucene Extenssions
	Chaining filters
		The contrib directory contains an interesting meta-filter in the misc project, contributed by Kelvin Tan, which chains other filters together and performs AND, OR, XOR, and ANDNOT bit operations between them. ChainedFilter, like the built-in CachingWrapperFilter, isn’t a concrete filter; it combines a list of filters and performs a desired bit-wise operation for each successive filter, allowing for sophisticated combinations.
		Depending on your needs, the same effect can be obtained by combining query clauses into a BooleanQuery or using FilteredQuery (see section 6.4.3). Keep in mind the performance caveats to using filters; and, if you’re reusing filters without changing the index, be sure you’re using a caching filter. ChainedFilter doesn’t cache, but wrapping it in a CachingWrappingFilter will take care of that.
	Storing an index in Berkeley DB
	Synonyms from WordNet	
		Building the synonym index
			The Syns2Index program converts the WordNet Prolog synonym database into a standard Lucene index with an indexed field word and unindexed fields syn for each document. WordNet 3.0 produces 44,930 documents, each representing a single word; the index size is approximately 2.9MB, making it compact enough to load as a RAMDirectory for speedy access.
		Tying WordNet synonyms into an analyzer
	Fast memory-based indices
	XML QueryParser: Beyond “one box” search interfaces
		Using XmlQueryParser
		Extending the XML query syntax
	Surround query language
		Indexing spatial data
			PROJECTING THE GLOBE
				To compute distances, we first must “flatten” the globe using a mathematical process called projection, depicted in figure 9.5. This is a necessary precursor so that we can represent any location on the surface of the earth using an equivalent two-dimensional coordinate system.
		Searching spatial data
			FINDING THE NEAREST RESTAURANT
		Performance characteristics of Spatial Lucene
			MEMORY
			DENSITY OF RESULTS
			PERFORMANCE NUMBERIC
	Searching multiple indexes remotely
	Flexible QueryParser
	Odds and ends
	Summary
	

Using Lucene from other programming languages
	Port primer
	
	
Lucene administration and performance tuning
	Performance tuning
		Simple performance-tuning steps
			Do not reopen IndexWriter or IndexReader/IndexSearcher any more frequently than required. Share a single instance for a long time and reopen only when necessary.
		Testing approach
			There are a large set of built-in tasks and document sources to choose from. Extending the framework with your own task is straightforward. You simply write an algorithm (.alg) file, using a simple custom scripting language, to describe the test. Then run it like this: cd contrib/benchmark ant run-task -Dtask-alg=<file.alg> -Dtask.mem=XXXM That code prints great details about the metrics for each step of your test. Algorithm files also make it simple for others to reproduce your test results: you just send it to them and they run it!
			APPLES AND ORANGES
				To work around this, you could set mergeFactor to an enormous number, to turn off merging entirely. This will make the tests at least comparable, but just remember that the resulting numbers aren’t accurate in an absolute sense, because in a real application you can’t turn off merging. This is only worthwhile if you aren’t trying to compare the cost of merging in the first place. 
				The second issue is to make sure your tests include the time it takes to call close on the IndexWriter. During close, IndexWriter flushes documents, may start new merges, and waits for any background merges to finish. Try to write your algorithm files so that the CloseIndex task is included in the report.
		Tuning for index-to-search delay	
		Tuning for indexing throughput
		Tuning for search latency and throughput
	Threads and concurrency
		Using threads for indexing
			ThreadedIndexWriter, that extends IndexWriter and uses java.util.concurrent to manage multiple threads, adding and updating documents.
			When you use this class, you can’t reuse Document or Field instances, because you can’t control precisely when a Document is done being indexed.
		Using threads for searching
			Threads make reopening your searcher challenging, because you can’t close the old searcher until all searches are done with it, including iterating through the hitsafter IndexSearcher.search has returned. Beyond that, you may want to keep the old searcher around for long enough for all search sessions (the original search plus all follow- on actions like clicking through pages) to finish or expire. For example, consider a user who’s stepping through page after page of results, where each page is a new search on your server. If you suddenly swap in a new searcher in between pages, then the documents assigned to each page could shift, causing the user to see duplicate results across pages or to miss some results. This unexpected behavior can erode your user’s trust—pretty much the kiss of death for any search application. Prevent this by sending new pages for a previous search back to the original searcher when possible.
			SearcherManager, that hides the tricky details of reopening your searcher in the presence of multiple threads. It’s able to open readers either from a Directory instance, in cases where you don’t have direct access to the IndexWriter that’s making changes, or from an IndexWriter by obtaining a near-real-time reader
	Managing resource consumption
		Lucene’s disk usage depends on many factors. An index with only a single pure indexed, typical text field will be about one third of the total size of the original text. At the other extreme, an index that has stored fields and term vectors with offsets and positions, with numerous deleted documents plus an open reader on the index, with an optimize running, can easily consume 10 times the total size of the original text!
		Rather than increasing gradually with time, as you add documents to the index, disk usage will suddenly ramp up during a merge and then quickly fall back again once the merge has finished, creating a sawtooth pattern. The size of this jump corresponds to how large the merge was (the net size of all segments being merged). Furthermore, with ConcurrentMerge- Scheduler, several large merges can be running at once and this will cause an even larger increase of temporary disk usage.
		Here’s a coarse formula to estimate the final size based on the size of all text from the documents: 1/3 x indexed + 1 x stored +2 x term vectors For example, if your documents have a single field that’s indexed, with term vectors, and is stored, you should expect the index size to be around 3 1/3 times the total size of all text across all documents. Note that this formula is approximate.
		Here are other things that will affect transient disk usage:
			?? Open readers prevent deletion of the segment files they’re using. You should have only one open reader at a time, except when you are reopening it. Be sure to close the old reader!
			?? All segments that existed when the IndexWriter was first opened will remain in the directory, as well as those referenced by the current (in memory) commit point. If you commit frequently, less transient disk space will be used, but committing can be a costly operation, so this will impact your indexing throughput.
			?? If you frequently replace documents but don’t run optimize, the space used by old copies of the deleted documents won’t be reclaimed until those segments are merged.
			?? The more segments in your index, the more disk space will be used—more than if those segments were merged. This means a high mergeFactor will result in more disk space being used.
			?? Given the same net amount of text, many small documents will result in a larger index than fewer large documents.
			?? Don’t open a new reader while optimize, or any other merges, are running; doing so will result in the reader holding references to segments that would otherwise be deleted. Instead, open after you have closed or committed your IndexWriter.
			?? Do open a new reader after making changes with IndexWriter, and close the old one. If you don’t, the reader could be holding references to files that IndexWriter wants to delete, due to merging, which prevents the files from being deleted. Further, the existing reader will continue to work fine, but it won’t see the newly committed changes from the IndexWriter until it’s reopened.
			?? If you’re running a hot backup (see section 11.4), the files in the snapshot being copied will also consume disk space until the backup completes and you release the snapshot.
		File descriptors
			Suppose you’re happily tuning your application to maximize indexing throughput. You turned off compound file format. You cranked up mergeFactor and got awesome speedups, so you want to push it even higher. Unfortunately, there’s a secret cost to these changes: you’re drastically increasing how many files Lucene must hold open at once. At first you’re ecstatic about your changes; everything seems fine. Then, as you add more documents, the index grows, Lucene will need more and more open files when one day—BOOM!—you hit the dreaded “Too many open files” IOException, and the OS stops you dead in your tracks.
			Armed with your new knowledge about open file consumption, here are some simple tips to keep them under control while still enjoying your indexing performance gains:
				?? Increase the IndexWriter buffer (setRAMBufferSizeMB). The less often the writer flushes a segment, the fewer segments there will be in the index.
				?? Use IndexReader.reopen instead of opening a whole new reader. This is a big reduction on peak open file count.
				?? Reduce mergeFactor—but don’t reduce it so much that it substantially hurts indexing throughput.
				?? Consider reducing the maximum number of simultaneous merge threads. Do this by calling ConcurrentMergeScheduler.setMaxThreadCount.
				?? Optimize the index. A partial optimize, using the IndexWriter.optimize(int maxNumSegments) method, is a good compromise for minimizing the time it takes to optimize while still substantially reducing the number of segments in the index.
				?? Always budget for your peak usage. This is often when you’re opening and warming a new reader, before you’ve closed the old one.
				?? If you run indexing and searching from a single JVM, you must add up the peak open file count for both. The peak often occurs when several concurrent merges are running and you’re reopening your reader. If possible, close your writer before reopening your reader to prevent this “perfect storm” of open files.
				?? Double-check that all other code also running in the same JVM isn’t using too many open files—if it is, consider running a separate JVM for it.
				?? Double-check that you’re closing your old IndexReader instances. Do this if you find you’re still running out of file descriptors far earlier than you’d expect.
		Memory
			You’ve surely hit OutOfMemoryError in your Lucene application in the past? If you haven’t, you will, especially when many of the ways to tune Lucene for performance also increase its memory usage. So you thought: no problem, just increase the JVMs’ heap size and move on. Nothing to see here. You do that, and things seem fine, but little do you know you hurt the performance of your application because the computer has started swapping memory to disk. And perhaps a few weeks later you encounter the same error again.
			Managing memory usage is especially exciting, because there are two different levels of memory. First, you must control how the JVM uses memory from the OS. Second, you must control how Lucene uses memory from the JVM. And the two must be properly tuned together.
			You manage the JVM by telling it how large its heap should be. The option -Xms size sets the starting size of the heap and the option -Xmx size sets the maximum allowed size of the heap. In a production server environment, you should set both of these sizes to the same value, so the JVM doesn’t spend time growing and shrinking the heap. Also, if there will be problems reaching the max (e.g., the computer must swap excessively), you can see these problems quickly on starting the JVM instead of hours later (at 2 a.m.) when your application suddenly needs to use all the memory. The heap size should be large enough to give Lucene the RAM that it needs, but not so large that you force the computer to swap excessively. Generally you shouldn’t just give all RAM to the JVM: it’s beneficial to leave excess RAM free to allow the OS to use as its I/O cache.
			How can you tell if the computer is excessively swapping? Here are some clues:
				?? Listen to your hard drives, if your computer is nearby: they’ll be noticeably grinding away, unless you’re using solid-state disks.
				?? On Unix, run vmstat 1 to print virtual memory statistics, once per second. Then look for the columns for pages swapped in (typically si) and pages swapped out (typically so). On Windows, use Task Manager, and add the column Page Faults Delta, using the View > Select Columns menu. Check for high numbers in these columns (say, greater than 20).
				?? Ordinary interactive processes, like a shell or command prompt, or a text editor, or Windows Explorer, are not responsive to your actions.
				?? Using top on Unix, check the Mem: line. Check if the free number and the buffers number are both near 0. On Windows, use Task Manager and switch to the Performance tab. Check if the Available and System Cache numbers, under Physical Memory, are both near 0. The numbers tell you how much RAM the computer is using for its I/O cache.
				?? CPU usage of your process is unexpectedly low.
			To manage how Lucene, in turn, uses memory from the JVM, you first need to measure how much memory Lucene needs. There are various ways, but the simplest is to specify the -verbose:gc and -XX:+PrintGCDetails options when you run Java, and then look for the size of the total heap after collection. This is useful because it excludes the memory consumed by garbage objects that aren’t yet collected. If your Lucene application needs to use up nearly all of the memory allocated for the JVM’s maximum heap size, it may cause excessive GC, which will slow things down. If you use even more memory than that, you’ll eventually hit OutOfMemoryError.
			During indexing, one big usage of RAM is the buffer used by IndexWriter, which you can control with setRAMBufferSizeMB. Don’t set this too low as it will slow down indexing throughput. While a segment merge is running, some additional RAM is required, in proportion to the size of the segments being merged.
			Searching is more RAM intensive. Here are some tips to reduce RAM usage during searching:
				?? Optimize your index to purge deleted documents.
				?? Limit how many fields you directly load into the FieldCache, which is entirely memory resident and time consuming to load (as described in section 5.1). Try not to load the String or StringIndex FieldCache entries as these are far more memory consuming than the native types (int, float, etc.).
				?? Limit how many fields you sort by. The first time a search is sorted by a given field, its values are loaded into the FieldCache. Similarly, try not to sort on String fields.
				?? Turn off field norms. Norms encode index-time boosting, which combines field boost, doc boost, and length boost into a single byte per document. Even documents without this field consume 1 byte because the norms are stored as a single contiguous array. This quickly works out to a lot of RAM if you have many indexed fields. Often norms aren’t actually a big contributor to relevance scoring. For example, if your field values are all similar in length (e.g., a title field), and you aren’t using field or document boosting, then norms are not necessary. Section 2.5.3 describes how to disable norms during indexing.
				?? Use a single “catchall” text field, combing text from multiple sources (such as title, body, or keywords), instead of one field per source. This reduces memory requirements within Lucene and could also make searching faster.
				?? Make sure your analyzer is producing reasonable terms. Use Luke to look at the terms in your index and verify these are legitimate terms that users may search on. It’s easy to accidentally index binary documents, which can produce a great many bogus binary terms that would never be used for searching. These terms cause all sorts of problems once they get into your index, so it’s best to catch them early by skipping or properly filtering the binary content. If your index has an unusually large number of legitimate terms—for example, if you’re searching a large number of product SKUs—try specifying a custom termInfos- IndexDivisor when opening your IndexReader to reduce how many index terms are loaded into RAM. But note that this may slow down searching. There are so many trade-offs!
				?? Double-check that you’re closing and releasing all previous IndexSearcher/ IndexReader instances. Accidentally keeping a reference to past instances can quickly exhaust RAM and file descriptors and even disk usage.
				?? Use a Java memory profiler to see what’s using so much RAM.
	Hot backups of the index
		Creating the backup
			Note that Lucene’s index files are write-once. This means you can do an incremental backup by simply comparing filenames. You don’t have to look at the contents of each file, nor its last modified timestamp, because once a file is written and referenced from a snapshot, it won’t be changed. The only exception is the file segments. gen, which is overwritten on every commit, and so you should always copy this file. You shouldn’t copy the write lock file (write.lock). If you’re overwriting a previous backup, you should remove any files in that backup that aren’t listed in the current snapshot, because they are no longer referenced by the current index.
			you can easily make a “hot backup” of the index, so that you create a consistent backup image, with just the files referenced by the most recent commit point, without closing your writer. No matter how long the copying takes, you can still make updates to the index. The approach is to use the SnapshotDeletionPolicy, which keeps a commit point alive for as long it takes to complete the backup. Your backup program can take as long as it needs to copy the files. You could throttle its I/O or set it to low process or I/O priority to make sure it doesn’t interfere with ongoing searching or indexing. You can spawn a subprocess to run rsync, tar, robocopy, or your favorite backup utility, giving it the list of files to copy. This can also be used to mirror a snapshot of the index to other computers.
			SnapshotDeletionPolicy has a couple of small limitations:
				?? It only keeps one snapshot alive at a time. You could fix this by making a similar deletion policy that keeps track of more than one snapshot at a time.
				?? The current snapshot isn’t persisted to disk. This means if you close your writer and open a new one, the snapshot will be deleted. So you can’t close your writer until the backup has completed. This is also easy to fix: you could store and load the current snapshot on disk, then protect it on opening a new writer. This would allow the backup to keep running even if the original writer is closed and new one opened.
		Restoring the index
			Here are the steps to follow when restoring an index:
				1 Close any existing readers and writers on the index directory, so the file copies will succeed. In Windows, if there are still processes using those files, you won’t be able to overwrite them.
				2 Remove all existing files from the index directory. If you see an “Access is denied” error, double-check step 1.
				3 Copy all files from your backup into the index directory. Be certain this copy doesn’t encounter any errors, like a disk full, because that’s a sure way to corrupt your index.
				4 Speaking of corruption,
	Common errors
		Lucene is wonderfully resilient to most common errors. If you fill up your disk, or see an OutOfMemoryException, you’ll lose only the documents buffered in memory at the time. Documents already committed to the index will be intact, and the index will be consistent. The same is true if the JVM crashes, or hits an unhandled exception, or is explicitly killed, or if the OS crashes or the electricity to the computer is suddenly lost.
		If you see a LockObtainFailedException, that’s likely because there’s a leftover write.lock file in your index directory that wasn’t properly released before your application or JVM shut down or crashed. Consider switching to NativeFSLockFactory, which uses the OS provided locking (through the java.nio.* APIs) and will properly release the lock whenever the JVM exits normally or abnormally. You can safely remove the write.lock file, or use the IndexReader.unlock static method to do so. But first be certain there’s no writer writing to that directory!
		If you see AlreadyClosedException, double-check your code: this means you’re closing the writer or reader but then continuing to use it.
		Index corruption
		Repairing an index
	Summary
		


Case studies
Case study 1: Krugle
	Introducing Krugle
	Appliance architecture
		we use separate JVMs for the data processing/indexing tasks and the searching/browsing tasks. This provides us with better controlled memory usage, at the cost of some wasted memory.
		On a typical appliance, we have two 32-bit JVMs running, each with 1.5 GB of memory. One other advantage to this approach is that we can shut down and restart each JVM separately, which makes it easier to do live upgrades and debug problems.
		Finally, we tune the disks being used to avoid seek contention. There are two drives devoted to snapshots: while one is serving up the current snapshot, the other is being used to build the new snapshot.
	Search performance
	Parsing source code
	Substring searching
	Query vs. search
	Future improvements
		FieldCache memory usage
			For dates, the easy answer is to use longs instead of ISO date strings. The only trick is to ensure that they’re stored as strings with leading zeros so that they still sort in date order.
		Combining indexes
	Summary
	
	
Case study 2: SIREn
	Introducing SIREn
	SIREn’s benefits
